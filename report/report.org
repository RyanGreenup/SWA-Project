#+TITLE: Analysing Twitter for Ubisoft
:SETUP:
#+INFOJS_OPT: view:info toc:3
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
#+OPTIONS: tex:t
#+TODO: TODO IN-PROGRESS FIXME DONE
#+CATEGORY: TAD
:END:
:HTML:
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="style.css">
# Not embedding the HTML is faster, enable toggle-org-custom-inline-style when
    # you want that feature
#+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args: R :results output :session swaproj :dir ~/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/report/ :cache yes
:END:
:SlowDown:
# #+STARTUP: latexpreview
#+LATEX_HEADER: \usepackage{/home/ryan/Dropbox/profiles/Templates/LaTeX/ScreenStyle}
# #+LATEX_HEADER: \twocolumn
# [[/home/ryan/Dropbox/profiles/Templates/LaTeX/ScreenStyle.sty]]
:END:
:LaTeX:
#+latex_header: \usepackage[citestyle=numeric, bibstyle=numeric,hyperref=true,backref=true, maxcitenames=3,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \addbibresource{/home/ryan/Dropbox/Studies/Papers/references.bib}
#+latex_header: %%% TeX-command-extra-options: "-shell-escape"
:END:

* 8.1 Analysing the Relationship Between Friends and Followers for Twitter Users
** 8.1.1 Retrieve the posts from Twitter
relevant posts can be retrieved from twitter by utilising the =rtweet= package, packages can be loaded for use in **_R_** thusly:

#+NAME: lpac
#+CAPTION: Load the Packages for **/R/**
#+begin_src R :output none :results none
# Load Packages -----------------------------------------------------------
setwd("~/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/scripts/")

if (require("pacman")) {
  library(pacman)
} else{
  install.packages("pacman")
  library(pacman)
}

pacman::p_load(xts, sp, gstat, ggplot2, rmarkdown, reshape2,
               ggmap, parallel, dplyr, plotly, tidyverse,
               reticulate, UsingR, Rmpfr, swirl, corrplot,
               gridExtra, mise, latex2exp, tree, rpart,
               lattice, coin, primes, epitools, maps, clipr,
               ggmap, twitteR, ROAuth, tm, rtweet, base64enc,
               httpuv, SnowballC, RColorBrewer, wordcloud,
               ggwordcloud, tidyverse, boot)
#+end_src


The =rtweet= API will search for tweets that contain all the words of a query
regardless of uppercase or lowercase usage cite:kearney2019.

In order to leverage the /Twitter/ API it is necessary to use tokens provided through a /Twitter/ developer account:

#+NAME: tkn
#+CAPTION: Import the twitter tokens (redacted)
#+begin_src R
# Set up Tokens ===========================================================

options(RCurlOptions = list(
  verbose = FALSE,
  capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"),
  ssl.verifypeer = FALSE
))

setup_twitter_oauth(
  consumer_key = "*************************",
  consumer_secret = "**************************************************",
  access_token = "**************************************************",
  access_secret = "*********************************************"
)

# rtweet ==================================================================
tk <-    rtweet::create_token(
  app = "SWA",
  consumer_key    = "*************************",
  consumer_secret = "**************************************************",
  access_token    = "**************************************************",
  access_secret   = "*********************************************",
  set_renv        = FALSE
#+end_src

and hence all tweets containing a mention of /Ubisoft/ can be returned and saved to disk as shown in listing [[save]]:

#+NAME: save
#+CAPTION: Save the Tweets to the HDD as an ~rdata~ file
#+begin_src R
 n <- 1000
 tweets.company <- search_tweets(q = 'ubisoft', n = n, token = tk,
                                 include_rts = FALSE)
 save(tweets.company[,], file = "resources/Download_1.Rdata")
#+end_src

** 8.2.2 Count of Followers and Friends
In order to identify the number of users that are contained in the /tweets/ the
=unique()= function can be used to return a vector of names which can then be passed as an index to the vector of counts as shown in listing [[count]], this provides that 81.7% of the tweets are by unique users.

#+NAME: count
#+CAPTION: Return follower count of twitter posts
#+begin_src R
(users <- unique(tweets.company$name)) %>% length()
x <- tweets.company$followers_count[duplicated(tweets.company$name)]
y <- tweets.company$friends_count[duplicated(tweets.company$name)]

## > [1] 817
#+end_src


** 8.1.3 Summary Statistics
The average number of friends and followers from users who posted tweets mentioning /Ubisoft/ can be returned using the ~mean()~ as shown in listing [[mean]]
this provides that on average each user has 586 friends and 63,620 followers.

#+NAME: mean
#+CAPTION: Determine the average number of friends and followers
#+begin_src R
x<- rnorm(090)
y<- rnorm(090)
(xbar <- mean(x))
(ybar <- mean(y))

## > [1] 4295.195
## > [1] 435.9449
#+end_src

** 8.1.4 Above Average Followers
Each user can be compared to the average number of followers, by using a logical
operator on the vector (e.g. ~y > ybar~), this will return an output of logical
values. */R/* will coerce logicals into 1/0 values meaning that the mean value
will return the proportion of =TRUE= responses as shown in listing [[pyhat]]. This
provides that:

+ 2.4%  of the have identified have an above average *number of followers*.
+ 20.6% of the users identified have an above average *number of friends*.

#+NAME: pyhat
#+CAPTION: Calculate the proportion of users with above average follower counts
#+begin_src R
(px_hat <- mean(x>xbar))
(py_hat <- mean(y>ybar))

## > [1] 0.0244798
## > [1] 0.2729498
#+end_src


** 8.1.5 Bootstrap confidence intervals
*** a/b.) Generate a bootsrap distribution

A bootstrap assumes that the population is an infinitely large repetition of the
sample and may be produces with respect to follower counts by resampling with
replacement/repetition and plotted using the =ggplot2= library as deomonstrated
in listings [[btpop]] and [[btgg]] and shown in figure [[btpopfg]].

This shows that the population follower counts is a non-normal skew-right
distribution, which is expected because the number of friends is an integer value bound by zero cite:nist2013.

#+NAME: btpop
#+CAPTION: Bootstrapping a population from the sample.
#+begin_src R
## Resample the Data
(bt_pop <- sample(x, size = 10^6, replace = TRUE)) %>% head()

## > [1]   7 515 262 309 186 166
#+end_src

#+NAME: btgg
#+CAPTION:
#+begin_src r
## Make the Population
bt_pop_data <- tibble("Followers" = bt_pop)
ggplot(data = bt_pop_data, aes(x = Followers)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 35, col = "pink") +
  geom_density(col = "violetred2") +
  scale_x_continuous(limits = c(1, 800)) +
  theme_bw() +
  labs(x = "Number of Followers", y = "Density",
       title = "Bootstrapped population of Follower Numbers")

#+end_src

#+attr_html: :width 400px
#+attr_latex: :width 12cm
#+NAME: btpopfg
#+CAPTION: Histogram of the bootrapped population of follower counts
[[./Figures/BootStrap_Pop.png]]

*** c.) Estimate a Confidence Interval for the population mean Follower Counts
In order to perform a bootrap for the population mean value of follower counts it is necessary to:

1. Resample the data with replacement
   + i.e. randomly select values from the sample allowing for repetition
2. Measure the statistic of concern
3. Replicate this a sufficient number of times
   + i.e. Greater than or equal to 1000 times [[cite:davison1997][Ch. 5]]

This is equivalent to drawing a sample from a population that is infinitely large and constructed of repetitions of the sample. This can be performed in */R/* as shown in listing [[bt1s]].


#+NAME: bt1s
#+CAPTION: Confidence Interval of Mean Follower Count in Population
#+begin_src R
xbar_boot_loop <- replicate(10^3, {
  s <- sample(x, replace = TRUE)
  mean(s)
  })
quantile(xbar_boot_loop, c((1-0.97)/2, (1+0.97)/2))

##       1.5%      98.5%
##   588.4189 10228.7352
#+end_src

A 97% probability interval is such that a sample drawn from a population will contain the population mean in that interval 97% of the time, this means that it may be concluded with a high degree of certainty that the true population mean lies between 588 and 10228.

**** Alternative Approaches
If this data was normally distributed it may have been appropriate to consider
bootstrapping the standard error and using a $t$ distribution, however it is more appropriate to use a
percentile interval for skewed data such as this, in saying that however this method is not considered to be very accurate in the literature and is often too narrow. [[cite:hesterberg2015][Section 4.1]]

- It's worth noting that the normal $t$ value bootstrap offers no advantage over
  using a $t$ distribution (other than being illustrative of bootstrapping
  generally) [[cite:hesterberg2015][Section 4.1]]


  The =boot= package is a bootstrapping library common among authors in the data science sphere
  [[cite:james2013][p. 295]] [[cite:wiley2019][p. 237]] that implements
  confidence intervals consistent with work by Davison and Hinkley
  cite:ripley2020 in there texbook /Bootstrap Methods and their Application/.
 In this work it is provided that the $BC_{a}$ method of constructing confidence
  intervals is  superior to mere percentile
  methods in terms of accuracy [[cite:davison1997][Ch. 5]], a sentiment echoed in the literature. [[cite:carpenter2000,davison1997][Ch. 5]]

 Such methods can be implemented in */R/* by passing a function to the the =boot= call as shown in listing [[bootx]]. This provides a broader interval, providing that the true confidence interval could lie between 1079 and 16227 followers.

 #+NAME: bootx
 #+CAPTION: Bootstrap of population mean follower count implementing the $BC_{a}$ method
 #+begin_src r
xbar_boot <- boot(data = x, statistic = mean_val, R = 10^3)
boot.ci(xbar_boot, conf = 0.97, type = "bca", index = 1)

## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = xbar_boot, conf = 0.97, type = "bca", index = 1)
##
## Intervals :
## Level       BCa
## 97%   ( 1079, 16227 )
## Calculations and Intervals on Original Scale
## Warning : BCa Intervals used Extreme Quantiles
## Some BCa intervals may be unstable
## Warning message:
## In norm.inter(t, adj.alpha) : extreme order statistics used as endpoints
 #+end_src

# * References
# Remember, this is here for HTML and autocomplete, but latex uses biblatex for URL support
# bibliography:/home/ryan/Dropbox/Studies/Papers/references.bib
# I (Ryan) am managing this with zotero, please don't touch, I'll figure out how to sync the citations later or we can all just switch to a =.bib= file.
<<bibliography link>>
bibliography:./references.bib

<<bibliographystyle link>>
 bibliographystyle:unsrt

*** d.) Estimate a Confidence Interval for the population mean Friend Counts
A Confidence interval for the population mean friend counts may be constructed in a like wise fashion as shown in listings [[booty]]. This provides that the 97% confidence interval for the population mean friend count is between 384 and 502 (or 387 and 496 if the $BC_{a}$ method used, they're quite close and so the more conservative percentile method will be accepted).

#+NAME: booty
#+CAPTION: Bootstrap of population mean follower count
#+begin_src R
# d.) Estimate a Confidence Interval for the populattion mean Friend Count ===
# Using a Percentile Method #####################################################
ybar_boot_loop <- replicate(10^3, {
  s <- sample(y, replace = TRUE)
  mean(s)
  })
quantile(ybar_boot_loop, c(0.015, 0.985)

# Using BCA Method #############################################################
mean_val <- function(data, index) {
  X = data[index]
  return(mean(X))
}

xbar_boot <- boot(data = y, statistic = mean_val, R = 10^3)
boot.ci(xbar_boot, conf = 0.97, type = "bca", index = 1)


##     1.5%    98.5%
## 383.7619 501.5903
##
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = xbar_boot, conf = 0.97, type = "bca", index = 1)
##
## Intervals :
## Level       BCa
## 97%   (386.8, 496.7 )
## Calculations and Intervals on Original Scale
## Some BCa intervals may be unstable
#+end_src

** FIXME 8.1.6 Estimate a 97% Confidence Interval for the High Friend Count Proportion
:PROPERTIES:
:DIR:      /home/ryan/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/docs/
:END:
In order to bootstrap a confidence interval for the proportion of users with
above average follower counts, repeteadly draw random samples from an infinitely
large population composed entirely of the sample, and record the sampled
proportion. this can be acheived by resampling the observations of above and
below as shown in listing [[phat]].

This provides that:
 + The 97% confidence interval for the population proportion of users that have an above average number of friends is between 0.24 and 0.31.
  + i.e. The probability of any given sample containing the population mean within this interval would be 97%, although  that doesn't however mean that there is a 97% probability that this interval contains the value, merely that we may be 97% /confident/

   #+begin_comment
   i.e. 97% of samples drawn from a population will have the population mean inside the corresponding confidence interval, So if we intended to sample a population and draw a confidence interval, that confidence interval would be the probability of any given sample containing the population mean.

   Given a sample however it is not correct to then conclude that the confidence interval is the probability of that sample containing the value because we simply have no information on how good the sample is at predicting the population, it could be a really biased sample and we can't really know what \mu is anyway.

   All we know is that if many samples were drawn and one randomly selected, the probability of that sample containing the poulation in the interval would be 97%

   See [[attachment:ConfIntNotes.pdf][this Attachment]]
   #+end_comment

#+NAME: phat
#+CAPTION: Bootstrap of Proportion of Friends above average
#+begin_src R
# 8.1.6 High Friend Count Proportion -------------------------------------------
prop <- factor(c("Below", "Above"))
## 1 is above average, 2 is below
py_hat_bt <- replicate(10^3, {
  rs      <- sample(c("Below", "Above"),
                    size = length(y),
                    prob = c(py_hat, 1-py_hat),
                    replace = TRUE)
isabove <- rs == "Above"
mean(isabove)
})
quantile(py_hat_bt, c(0.015, 0.985))


##      1.5%     98.5%
## 0.2399021 0.3072215
## > > > . + > > >
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = py_hat_boot, conf = 0.97, type = "bca")
##
## Intervals :
## Level       BCa
## 97%   ( 0.2399,  0.3072 )
## Calculations and Intervals on Original Scale
#+end_src
** 8.1.7 Is the Number of Friends Independent to the Number of Followers
One method to determine whether or not the number of followers is independent of the number of friends is to bin the counts and determine whether or not the distribution of users across those counts is consistent with the hypothesis of independence.

*** Bin the Follower and Friend Categories
The counts may be binned by performing a logical interval test as shown in listing [[7bin]].

#+NAME: 7bin
#+CAPTION: Use Logical Test to Assign observations into bins
#+begin_src R
## Assign Categories
x_df <- data.frame(x)
x_df$cat[0       <= x_df$x & x_df$x < 100] <- "Tens"
x_df$cat[100     <= x_df$x & x_df$x < 1000] <- "Hundreds"
x_df$cat[1000    <= x_df$x & x_df$x < 2000] <- "1Thousands"
x_df$cat[2000    <= x_df$x & x_df$x < 3000] <- "2Thousands"
x_df$cat[3000    <= x_df$x & x_df$x < 4000] <- "3Thousands"
x_df$cat[4000    <= x_df$x & x_df$x < 5000] <- "4Thousands"
x_df$cat[5000    <= x_df$x & x_df$x < Inf] <- "5ThousandOrMore"

### Make a factor
x_df$cat <- factor(x_df$cat, levels = var_levels, ordered = TRUE)

### Determine Frequencies
(x_freq <- table(x_df$cat) %>% as.matrix())

## ** b) Find the Friend Count Frequency ===========================================
## Assign Categories
y_df <- data.frame(y)
y_df$cat[0       <= y_df$y & y_df$y < 100] <- "Tens"
y_df$cat[100     <= y_df$y & y_df$y < 1000] <- "Hundreds"
y_df$cat[1000    <= y_df$y & y_df$y < 2000] <- "1Thousands"
y_df$cat[2000    <= y_df$y & y_df$y < 3000] <- "2Thousands"
y_df$cat[3000    <= y_df$y & y_df$y < 4000] <- "3Thousands"
y_df$cat[4000    <= y_df$y & y_df$y < 5000] <- "4Thousands"
y_df$cat[5000    <= y_df$y & y_df$y < Inf]  <- "5ThousandOrMore"

### Make a factor
y_df$cat <- factor(y_df$cat, levels = var_levels, ordered = TRUE)

### Determine Frequencies
(y_freq <- table(y_df$cat) %>% as.matrix())
#+end_src

*** Find the Group frequency
These values may be tabluated in order to count the occurence of users among these categories as shown in listing [[7fr]] and table [[7frt]].

#+NAME: 7fr
#+CAPTION: Tabulate the binned counts for the distribution of users among among amount and status.
#+begin_src R
vals <- t(cbind(x_freq, y_freq))
rownames(vals) <- c("Followers.x", "followers.y")
vals

##             Tens Hundreds 1Thousands 2Thousands 3Thousands 4Thousands
## Followers.x  421      317         39         11          9          2
## followers.y  262      476         47         15          6          9
##             5ThousandOrMore
## Followers.x              18
## followers.y               2
#+end_src

#+NAME: 7frt
#+CAPTION: Table of Binned Friend and Follower counts, transposed relative to code.
|                      | **/Followers/** | **/Friends/** |
| /Tens/               |             421 |           262 |
| /Hundreds/           |             317 |           476 |
| /1 - Thousands/      |              39 |            47 |
| /2 - Thousands/      |              11 |            15 |
| /3 - Thousands/      |               9 |             6 |
| /4 - Thousands/      |               2 |             9 |
| /5 Thousand or More/ |              18 |             2 |

*** Find the Expected Counts under each group and test for independence
The expected count of each cell, under the assumption that the two metrics are
independent, will be the proportion users per bracket multiplied by the number
of users in that status group. This implies that any cell will be:

+ the product of the row sum, multiplied by the column sum divided by the number of counts.

This can be equivalently expressed as an outer product as shown in equation
[[eqref:eq:1]], in */R/* this operation is denoted by the =%o%= operator, which is
shorthand for the =outer()= function, this and other summary statistics may be
evaluated as shown in listing [[smst]].

The outer product is such that:


$$
\mathbf{u} \otimes \mathbf {v} =\mathbf {u} \mathbf {v} ^{\textsf {T}}={\begin{bmatrix}u_{1}\\u_{2}\\u_{3}\\u_{4}\end{bmatrix}}{\begin{bmatrix}v_{1}&v_{2}&v_{3}\end{bmatrix}}={\begin{bmatrix}u_{1}v_{1}&u_{1}v_{2}&u_{1}v_{3}\\u_{2}v_{1}&u_{2}v_{2}&u_{2}v_{3}\\u_{3}v_{1}&u_{3}v_{2}&u_{3}v_{3}\\u_{4}v_{1}&u_{4}v_{2}&u_{4}v_{3}\end{bmatrix}}.
$$

This means the matrix of expected frequencies can be expressed as an outer product thusly:

#+NAME: rschi
\begin{align}
\mathbf{\vec{e}}= \frac{1}{n} \times \begin{bmatrix} \sum^{n}_{j= 1} \left[ o_{1j} \right] \\  \sum^{n}_{j= 1}
\left[ o_{2j} \right]  \\ \sum^{n}_{j= 1} \left[ o_{3j} \right]   \\
\sum^{n}_{j= 1} \left[ o_{4j} \right]  \\ \vdots  \\
\sum^{n}_{j= 1} \left[ o_{nj} \right]     \end{bmatrix}
\begin{bmatrix}  \sum^{n}_{j= 1} \left[ o_{i1}  \right] \\  \sum^{n}_{j= 1}
\left[ o_{i2}  \right] \\ \sum^{n}_{j= 1} \left[ o_{i3}  \right] \\ \cdots \\
\sum^{n}_{j= 1} \left[ o_{in}  \right]   \end{bmatrix}  ^{\normalsize \mathrm{T}} \label{eq:1}
\end{align}

#+NAME: smst
#+CAPTION: Calculate Expected frequency of values under the assumption of independence.
#+begin_src R
## ***** Calculate Summary Stats
n <- sum(vals)
bracket_prop <- colSums(vals) / n
metric_prop  <- rowSums(vals) / n
o <- vals
e <- rowSums(vals) %o% colSums(vals) / n
chi_obs <- sum((e-o)^2/e)
#+end_src

**** Testing Independence
In order to test whether or not the distribution of users among brackets is
independent of being a follower or friend a $\chi^{2}$ test may be used, this
can be evaluated from a model or simulated, in */R/*, the simulated test is
shown in listing [[chib]], this provides a $p$ -value < 0.0005, which means that the hypothesis of independence may be rejected with a high degree of certainty.

#+NAME: chib
#+CAPTION: Chi-Square testing for independence between friend and follower bin categories.
#+begin_src R
chisq.test(vals, simulate.p.value = TRUE)


## 	Pearson's Chi-squared test with simulated p-value (based on 2000
## 	replicates)
##
## data:  vals
## X-squared = 88.109, df = NA, p-value = 0.0004998
#+end_src

***** From First Principles
The $\chi^{2}$ statistic may be performed from first principles by randomly
sampling the values at the rate at which they occured, tabulating those counts, measuring the $\chi^{2}$ -value and then repeating this many times.

Because the samples are random they must be independent and average number of
positives is hence an estimate for the /FPR/, which is in turn an estimate for
the $p$ -value. This technique is demonstrated in listing [[chif]], the p-value
being returned as 0.0004, this value is consistent with the value produced by
*/R/*'s built in =chisq.test= function and so is accepted.

#+NAME: chif
#+CAPTION: Performing a $\chi^{2}$ statistic from first principles
#+begin_src R
## ***** Create Vectors of factor levels
brackets <- unique(x_df$cat)
metrics <- c("follower", "friend")

## ***** Simulate the data Assuming H_0
## I.e. assuming that the null hypothesis is true in that
## the brackets assigned to followers are independent of the friends
## (this is a symmetric relation)

s <- replicate(10^4,{
  ## Sample the set of Metrics
  m <- sample(metrics, size = n, replace = TRUE, prob = metric_prop)

  ## Sample the set of Brackets (i.e. which performance bracket the user falls in)
  b <- sample(brackets, size = n, replace = TRUE, prob = bracket_prop)

  ## Make a table of results
  o <- table(m, b)
  o

  ## Find What the expected value would be
  e_sim <- t(colSums(e) %o% rowSums(e) / n)

  ## Calculate the Chi Stat
  chi_sim <- sum((e_sim-o)^2/e_sim)
  chi_sim

  ## Is this more extreme, i.e. would we reject null hypothesis?
  chi_sim > chi_obs

})

mean(s)

## [1] 4e-04
#+end_src
*** FIXME Conclusion
The $p$ -value measures the probability of rejecting the null hypothesis when it is true, i.e. the probability of a detecting a /false positive/, a very small $p$ -value is hence good evidence that the null hypothesis should be rejected (because doing so would unlikely to be a mistake).

In saying that however the $p$ -value is distinct from the /power/ statistic, which is a measure of //the probability of accepting the alternative hypothesis/ when it is true, a low $p$ -value is not a measurement of the probability of being correct.

Hence me way conclude, with a high degree of certainty, that the follower and friend counts are not independent of one another.
* 8.2 Finding Themes in tweets
** 8.2.8 Find Users with Above Average Friend Counts
Users with Above average Friend Counts can be identified by filtering the tweets
data frame for two conditions:

1. non-duplicated =user-id=
2. =friend_count= greater than average

This can be acheived easily using the =dplyr= package as shown in [[hfdp]], the top 20 of these users are shown in table [[hfls]] of the appendix

#+NAME: hfdp
#+CAPTION: Use =dplyr= to Filter for Users with a high Friend Count
#+begin_src R
select <- dplyr::select
filter <- dplyr::filter
interested_vars <- c("user_id", "friends_count")
(friend_counts <- tweets.company %>%
  select(interested_vars) %>%
  filter(!duplicated(user_id)))

(high_friends <- friend_counts %>%
  filter(friends_count > mean(friends_count, na.rm = TRUE)))

## Export Friends List
write.csv(high_friends[order(
  high_friends$friends_count,
  decreasing = TRUE),], file = "/tmp/highfriend.csv")
#+end_src


** 8.2.9 Find Users with Below Average Friend Counts
Users with high friends may be determined by a similar method (or by taking the complement of the high friends) as shown in listing [[lfcd]], the lowest 20 of these users are shown in table [[lftb]] of the appendix.

#+NAME: lfcd
#+CAPTION: Use =dplyr= to Filter for Users with a low Friend Count
#+begin_src R
(low_friends <- friend_counts %>%
  filter(friends_count <= mean(friends_count, na.rm = TRUE)))

 low_friends <- low_friends[order(
   low_friends$friends_count,
   decreasing = TRUE),]

## Export Users
write.csv(low_friends[order(
  low_friends$friends_count,
  decreasing = FALSE),], file = "/tmp/lowfriend.csv")
#+end_src

** 8.2.10 Find the /Tweets/ corresponding to users with high or low friend counts
The tweets corresponding to users with high and low friend counts can be
identified by filtering the dataframe based on the friend count and using that
to the index the tweets from the data frame [fn:1], alternatively it is possible
to test whether or not the ID of a user appears in the high or low vector
set using the =%in%= operator as shown in listing [[8210]].

#+NAME: 8210
#+CAPTION: Identify tweets corresponding to users with high and low friend counts
#+begin_src R
## Method 1
friend_test <- tweets.company$friends_count > mean(tweets.company$friends_count)
tweets_high <- tweets.company$text[friend_test]
tweets_low <- tweets.company$text[!friend_test]

## Method 2                                                                 :15b5a74:
## top part is the high friends and the low part is the low friends

tweets_high <- tweets.company$text[tweets.company$user_id %in%  high_friends$user_id]
tweets_low  <- tweets.company$text[tweets.company$user_id %in%  low_friends$user_id]
tweets <- c(tweets_high, tweets_low)
#+end_src
** 8.2.11 Clean the tweets
*** Create a Corpus Object
In order to clean the tweets it is necessary to create a corpus object as shown in listing [[cpmk]], it is possible to pass a dataframe source in order to include the user ID, this isn't strictly necessary however because the =tm= package preserves order when performing transformations.

#+NAME: cpmk
#+CAPTION: Create a Corpus from the tweets
#+begin_src R
tweet_source <- tm::VectorSource(doc_id = tweets$user_id, text = tweets$text)
tweet_corpus <- tm::Corpus(x = tweet_source)
#+end_src

Next it is necessary to choose an enoding, a primary consideration of this is whether or not the use of /emoji/ characters will influence the model performance. There is research to suggest that Emoji's can be used as predictive features cite:lecompte2017 and that they can improve /sentiment analysis/ models cite:shiha2017 that implement a /bag of words/ approach. For these reasons /emoji/ characters will be preserved and [[http://www.utf-8.com/][UTF-8]] implemented.

In order to encode the data as /UTF-8/, the =iconv= function can be used as shown in listing [[icv]].

#+NAME: icv
#+CAPTION: Encode the Data as UTF-8
#+begin_src R
encode <- function(x) {
    iconv(x, to = "UTF-8")
#    iconv(x, to = "latin1")
  #  iconv(x, to = "ASCII")
}

tweet_corpus <- tm_map(x = tweet_corpus, FUN = encode)
tweet_corpus_raw <- tweet_corpus
#+end_src

*** Process the tweets
Before analysis the tweets should be modified to remove characters that may interfere with categorising words, this is referred to as cleaning, in particular the following should be implemented:

1. Remove URL's
2. Remove Usernames
3. remove numbers
4. remove punctuation
5. remove whitespace
6. case fold all characters to lower case
7. remove a set of stop words
8. reduce each word to its stem

In particular it is important to reduce words to lower case before removing stop words otherwise an unorthodox use of capitalisation may prevent the word from being removed throughout.

The stop word =ubisoft= will also be used, this was the query term so it's expected to turn up at a very high frequency, the words =can= and ='s= also occured quite frequently and so were removed.

The cleaning can be implemented by mapping functions over the corpus, which is fundamentally a list, this can be performed via the =tm_map= function as shown in listing [[cltw]].



#+NAME: cltw
#+CAPTION: Use the =tm_map= function to clean the tweets
#+begin_src R
mystop <- c(stopwords(), "’s", "can", "ubisoft", "@ubisoft", "#ubisoft")# <<stphere>>

clean_corp <- function(corpus) {
  ## Remove URL's
  corpus <- tm_map(corpus,content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+","",x)))
  ## Remove Usernames
  corpus <- tm_map(corpus,content_transformer(function(x) gsub("@\\w+","",x)))
  ## Misc
  corpus <- tm_map(corpus, FUN = removeNumbers)
  corpus <- tm_map(corpus, FUN = removePunctuation)
  corpus <- tm_map(corpus, FUN = stripWhitespace)
  corpus <- tm_map(corpus, FUN = tolower)
  corpus <- tm_map(corpus, FUN = removeWords, mystop)
  ## stopwords() returns characters and is fead as second argument
  corpus <- tm_map(corpus, FUN = stemDocument)
  return(corpus)
}

tweet_corpus_clean <- clean_corp(tweet_corpus)
#+end_src

** 8.2.12 Display the first two tweets before/after processing
The tweets can be viewed from inside the corpus by selecting with the =[= function [fn:2] as demonstrated in listing [[ispt]], the first /tweet/ was rendered empty by the processing and the following two tweets were:

+ */Pre-Processing/*
    #+begin_quote
   + "Today was the first time in over a month that I have gone 24 hours without
    checking the coronavirus death toll. Thanks =@Ubisoft=."

   + "@btwimskrank =@TheDivisionGame= =@UbiMassive= =@Ubisoft= =@jgerighty= =@hamishbode=
    =@Tideman92= =@janeyo_jane= =@slimjd= Very odd... I'll even post a video about it."
    #+end_quote
+ */Post-Processing/*
    #+begin_quote
    + "today first time month gone hour without check coronavirus death toll thank"

    + "odd ill even post video"
    #+end_quote

#+NAME: ispt
#+CAPTION: Load the Packages for **/R/**
#+begin_src R
tweet_corpus_raw[[1]]$content
tweet_corpus_clean[[1]]$content
tweet_corpus_raw[[2]]$content
tweet_corpus_clean[[2]]$content
tweet_corpus_raw[[3]]$content
tweet_corpus_clean[[3]]$content
#+end_src

** 8.2.13 Create a Term Document Matrix
*** Apply Weighting Manually
**** Create Term Documen Matrix
A term Document matrix (and it's transpose) can be constructed from a =corpus= using the
=tm:TermDocumentMatrix= function as shown in listing [[tdm]].

#+NAME: tdm
#+CAPTION: Load the Packages for **/R/**
#+begin_src R
tweet_matrix_tdm <- TermDocumentMatrix(tweet_corpus_clean)
tweet_matrix_dtm <- DocumentTermMatrix(tweet_corpus_clean)
#+end_src

**** Apply TF-IDF Weighting
Weighted term frequency is defined as shown in equation eqref:tfidf, where:

+ $f_{d,t}$ is the frequency of a given term $t$ in the a document $d$
+ $w_{d,t}$ is the weight of a given term $t$ in the a document $d$
+ $N$ is the number of documents
+ $f_{t}$ is the number of documents containing $t$

\begin{equation}\begin{aligned}
w_{d, t} &=\mathrm{TF}_{t} \times \mathrm{IDF}_{d, t} \\
&=\log _{e}\left(f_{d, t}+1\right) \log _{e}\left(\frac{N}{f_{t}}\right) \label{tfidf}
\end{aligned}\end{equation}

This would requre multiplying each term of each row of the $\mathrm{TF}_{t}$ matrix by the corresponding vector element of $\mathrm{IDF}_{d,t}$, this can be implemented by taking the matrix product of a diagonalised matrix, this is shown in listing [[mytfidf]].

#+NAME: mytfidf
#+CAPTION: Apply TF-IDF Weigting
#+begin_src R
N <- nrow(as.matrix(tweet_matrix_dtm))   # Number of Documents
ft=colSums(as.matrix(tweet_matrix_dtm) > 0) #in how many documents term t appeared in,

TF <- log(as.matrix(tweet_matrix_dtm) + 1)  # built in uses log2()
IDF <- log(N/ft)

tweet_weighted           <- TF %*% diag(IDF)
colnames(tweet_weighted) <- colnames(tweet_matrix_dtm)
#+end_src

There is however a function built in to the =tm= package that will weight term document matrices and this will instead be implented to analyse the data because it will produce more maintainable code.

*** Apply Weighting with Built in Function
In order to create a term document matrix (and its transpose) with TF-IDF weighted values, the =weighting= argument may be specified as =weightTfIdf= by passing an appropriate list to the =control= argument of the =TermDocumentMatrix=, as shown in listing [[btdm]]

#+NAME:btdm
#+CAPTION: Create a Document Term Matrix by transforming a Term Document Matrix
#+begin_src R
tweet_weighted_tdm <- tm::TermDocumentMatrix(x = tweet_corpus_clean, control = list(weighting = weightTfIdf))
tweet_weighted_dtm <- as.DocumentTermMatrix()  %>%
                        as.matrix()
#+end_src

*** Remove Empty Documents
Empty Documents may be removed from the matrix by a logical test as shown in listing  [[emp]] [fn:3] this provides that 328 documents were empty following the processing. A summary of the first rows and columns of this matrix, following the removal of empty documents, is provided in table [[tfidf]] of the appendix.

#+NAME:emp
#+CAPTION: Load the Packages for **/R/**
#+begin_src R
null = which(rowSums(as.matrix(tweet_weighted_dtm)) == 0)
rowSums(as.matrix(tweet_weighted_dtm)==0)

if(length(null)!=0){
  tweet_weighted_dtm = tweet_weighted_dtm[-null,]
}

length(null)

## [1] 328
#+end_src
** 8.2.14 How many Clusters are there
*** Use Cosine Distance
In order to consider clustering, it can be more effective to consider the distance between the weigted documents in terms of cosine distance, the cosine distance can be calculated from the euclidean distance using the identity shown in eqref:cos, and this can be performed in */R/* by taking the matrix product of a diagonalised matrix as shown in listing [[dist]].

\begin{align}
\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)= \left| \left| \mathbf{X}-\mathbf{Y} \right| \right| \\
&= \sqrt{\sum^{n}_{i= 1}   \left[ \left( x_i-y_i \right)^2 \right] } \\
\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)^2&= \sum^{n}_{i= 1}  \left[ \left( x_i-y_i \right)^2 \right] \\
&= \sum^{n}_{i= 1}   \left( x^2 \right)+  \sum^{n}_{i= 1}   \left( y_i^2 \right)+ 2 \sum^{n}_{i= 1}   \left( x_iy_i \right) \\
&= 1+ 1 +  2 \times  \frac{\sum^{n}_{i= 1}   \left( x_iy_i \right)}{\left( 1 \right) }\\
&= 2+ 2\times \frac{\sum^{n}_{i= 1}   \left( x_iy_i \right)}{\left| \left| \mathbf{X} \right| \right|\times \left| \left| \mathbf{Y} \right| \right|}\\
&= 2+ 2 \cos\left( \mathbf{X}, \mathbf{Y} \right)\\
\ \\
& \implies  \left( 1- \cos\left( \mathbf{X}, \mathbf{Y} \right) \right) = \frac{\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)}{2} \label{cos}
\end{align}

#+NAME: dist
#+CAPTION: Load the Packages for **/R/**
#+begin_src R
norm.tweet_weighted_dtm = diag(
                    1/sqrt(rowSums(tweet_weighted_dtm^2))
                    ) %*% tweet_weighted_dtm
D =dist(norm.tweet_weighted_dtm, method = "euclidean")^2/2
#+end_src

*** Project into Euclidean Space
The cosine distance however is not appropriate to perform clustering on and so instead should be projected back into euclidean space, this can be acheived using /Multi-Dimensional Scaling/ via the =cmdscale= functionas shown in listing [[mds]]. The distance is a measure of between document distance so the number of dimensions should correspond to the number of documents, however, if there are zero-value eigenvalues, these dimensions won't help explain the data in the projection, hence the number of eigenvalues has been used as the dimension of projection in this case.

#+NAME: mds
#+CAPTION: Load the Packages for **/R/**
#+begin_src R
l <- min(nrow(tweet_weighted_dtm),
         ncol(tweet_weighted_dtm))
ev <- eigen(tweet_weighted_dtm[1:l, 1:l])
k <- (ev$values != 0) %>% sum()

mds.tweet_weighted_dtm <- cmdscale(D, k=k) #TODO What should K be? see issue #10
#+end_src

*** Measure Within Cluster Variance
In order to determine the appropriate number of clusters, the within cluster
variance can be measurd, the number of clusters at which this value ceases to
decrease is indicative of a potentially appropriate number of clusters. This is
implemented in listing [[ssw]] and shown in figure [[sswp]].

#+NAME: ssw
#+CAPTION: Use a loop to evaluate the performace of various cluster models, plot this with /ggplot2/
#+begin_src R
set.seed(271)
n = 15 # Assume it bends at 7 clusters
SSW = rep(0, n)
for (a in 1:n) {
  K = kmeans(mds.tweet_weighted_dtm, a, nstart = 20)
  SSW[a] = K$tot.withinss
  paste(a*100/n, "%") %>% print()
}
SSW


SSW_tb <- tibble::enframe(SSW)
ggplot(SSW_tb, aes(x = name, y = value)) +
  geom_point(col = "#Cd5b45", size = 5) +
  geom_line(col = "#Da70d6") +
  geom_vline(xintercept = 7, lty  = 3, col = "blue") +
  theme_bw() +
  labs(x = "Number of Clusters",
       y = "Within Cluster Sum of Square Distance",
       title = "Within Cluster Variance across Clusters")
#+end_src

Figure [[sswp]] Indicates a sudden stop of decrease in variance at 7 clusters and following that the within cluster variance begins to decrease at a slightly slower rate. For this reason 7 could be an appropriate candidate for the number of clusters, however the minimal amount of change in the within-cluster variance indicates that the data is most likely not clustered at all.


#+NAME: sswp
#+CAPTION: Plot of the Within Cluster Variance of the tweets using cosine distance (projected into Euclidean Space)
#+attr_html: :width 400px
#+attr_latex: :width 10cm
[[./Figures/Q14WithinClusterVariance.png]]

** 8.2.15 Find the Number of tweets in each cluster
Moving forward we'll use 3 clusters, 7 is too large and a smaller number will likely be more effective at categorising the data (particularly given that the stratification of the data appears to be quite limited from figure [[sswp]]). The number of tweets in a cluster may be measured by using the =table= function as shown in listing [[tab]] and table [[tabo]].

#+NAME: tab
#+CAPTION: The =table= function can count the number of tweets per cluster.
#+begin_src R
K = kmeans(mds.tweet_weighted_dtm, 3, nstart = 20)
table(K$cluster)
#+end_src

#+NAME: tabo
#+CAPTION: Number of tweets in each cluster identified by $k$ means clustering
| Cluster         |  1 |  2 |   3 |
|-----------------+----+----+-----|
| =#= of /tweets/ | 77 | 58 | 537 |

* Appendix
** Users with High Friend Count

#+NAME: hfls
#+CAPTION: User ID and Friend Count of users with above highest friend count in sample
|         */User ID/* | */Friend Count/* |
|---------------------+------------------|
|           274488119 |             8752 |
|           743771665 |             5002 |
|          1036014247 |             4999 |
|          2281452613 |             4992 |
|          1554453560 |             4958 |
|  981233818408570880 |             4944 |
|  931765564388921344 |             4836 |
|           807405140 |             4710 |
| 1112579152970842112 |             4514 |
|          2441577446 |             4322 |
|           552692862 |             4229 |
|  956297007127252992 |             3976 |
|            22493896 |             3675 |
|           255922782 |             3500 |
| 1067409881332936709 |             3312 |
|            27998570 |             3210 |
|  715118521555017728 |             3099 |
|          2356170174 |             2885 |
|          2372688230 |             2880 |
|          1868357425 |             2719 |
** Users with Low Friend Count
#+NAME: lftb
#+CAPTION: User ID and Friend Count of users with above highest friend count in sample
 | */User ID/*           | */Friend Count/* |
 |-----------------------+------------------|
 | 1254280995592966145   |                0 |
 | 875126772978913280    |                0 |
 | 1254256124217319425   |                0 |
 | 1250219450210480128   |                0 |
 | 1214921087328411648   |                0 |
 | 1254115699628421120   |                0 |
 | 1217600080376520704   |                0 |
 | 1253480062453600257   |                0 |
 | 1254178435502571521   |                0 |
 | 1251955545092718592   |                0 |
 | 1106864828700712960   |                0 |
 | 1160744587620524032   |                0 |
 | 1254256536710504448   |                1 |
 | 1129040408384868352   |                1 |
 | 1254121201871589376   |                1 |
 | 1248687797755658243   |                2 |
 | 1210265263867932675   |                2 |
 | 3380784928            |                3 |
 | 1177274165239275520   |                3 |
 | 54645521              |                3 |

** TF-IDF Matrix

#+NAME: tfidf
#+CAPTION: Document Term Matrix of first 6 documents and first 6 Words
| **check** | **coronavirus** | **death** | **first** | **gone** | **hour** |
|-----------+-----------------+-----------+-----------+----------+----------|
|     0.615 |           0.747 |     0.830 |     0.596 |    0.747 |    0.698 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
** Relevant XKCD
#+NAME: xkcd
#+CAPTION: /xkcd/ =#= 297
#+attr_html: :width 400px
#+attr_latex: :width 7cm
[[./Figures/lisp_cycles.png]]
* Footnotes

[fn:3] It is important not to filter based the logic of an empty vector, because
otherwise an empty vector will returned, hence the if statement in listing [[emp]].

[fn:2] The =[= function is actually shorthand for =Extract()=, most things in
*/R/* are functions, this is similar to /LISP/ and has to do with the origins of
the language, e.g. ~sum(1:10) == (sum (1:10))~, also relevant see the relevant
xkcd in figure [[xkcd]].




[fn:1] This works because the =tm= package preserves the order of the data, this can be confirmed by using a dataframe source as opposed to a vector source (e.g. in listing [[cpmk]]) and comparing the ID's before/after transformation.
# see ./scripts/q2.R
