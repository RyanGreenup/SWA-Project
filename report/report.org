#+TITLE: Analysing Twitter for Ubisoft
:SETUP:
#+INFOJS_OPT: view:info toc:3
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
#+OPTIONS: tex:t
#+TODO: TODO IN-PROGRESS FIXME DONE
#+CATEGORY: TAD
:END:
:HTML:
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="style.css">
# Not embedding the HTML is faster, enable toggle-org-custom-inline-style when
    # you want that feature
#+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args: R :results output :session swaproj :dir ~/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/report/ :cache yes
:END:
:SlowDown:
# #+STARTUP: latexpreview
#+LATEX_HEADER: \usepackage{/home/ryan/Dropbox/profiles/Templates/LaTeX/ScreenStyle}
# #+LATEX_HEADER: \twocolumn
# [[/home/ryan/Dropbox/profiles/Templates/LaTeX/ScreenStyle.sty]]
:END:
:LaTeX:
#+latex_header: \usepackage[citestyle=numeric, bibstyle=numeric,hyperref=true,backref=true, maxcitenames=3,url=true,backend=biber,natbib=true]{biblatex}
#+latex_header: \addbibresource{/home/ryan/Dropbox/Studies/Papers/references.bib}
#+latex_header: %%% TeX-command-extra-options: "-shell-escape"
:END:


* Submission Statement

| */Name/*        | */Student Number/* |
| Ryan Greenup    |           17805315 |
| Ben Britz       |           19579320 |
| Kieran Keith    |           19723017 |
| Navid Niknezhad |           19636740 |
| Hassan Rajab    |           19413192 |

Contribution Statistics are reflected by the =git= commit log [[https://github.com/RyanGreenup/SWA-Project/commits/master][Available here]],
all work was implemented either through code or /GitHub/ issues and so the
repository is a good resource for contribution metrics.

By including this statement, we the authors of this work, verify that:

+ We hold a copy of this assignment that we can produce if the original is lost or damaged.
+ We hereby certify that no part of this assignment/product has been copied from
  any other student's work or from any other source except where due
  acknowledgement is made in the assignment.
+ No part of this assignment/product has been written/produced for us by another
  person except where such collaboration has been authorised by the subject
  lecturer/tutor concerned.
+ We are aware that this work may be reproduced and submitted to plagiarism
  detection software programs for the purpose of detecting possible plagiarism
  (which may retain a copy on its database for future plagiarism checking).
+ We hereby certify that we have read and understand what the School of
  Computing and Mathematics defines as minor and substantial breaches of
  misconduct as outlined in the learning guide for this unit.


* 8.1 Analysing the Relationship Between Friends and Followers for Twitter Users
** 8.1.1 Retrieve the posts from Twitter
To analyze tweets on/Ubisoft/ it will be necessary to
download them in a structure that may be interpreted by a given programming
language, this analysis will be done using */R/*, and the by utilizing the =rtweet= package will be used to interface with the /Twitter/ API. The =rtweet= package can be automatically installed and loaded in */R/* using the =pacman= package, this is demonstrated in listing [[lpac]].

#+NAME: lpac
#+CAPTION: Load the Packages for **/R/**
#+begin_src R :output none :results none
# Load Packages -----------------------------------------------------------
setwd("~/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/scripts/")

if (require("pacman")) {
  library(pacman)
} else{
  install.packages("pacman")
  library(pacman)
}

pacman::p_load(xts, sp, gstat, ggplot2, rmarkdown, reshape2,
               ggmap, parallel, dplyr, plotly, tidyverse,
               reticulate, UsingR, Rmpfr, swirl, corrplot,
               gridExtra, mise, latex2exp, tree, rpart,
               lattice, coin, primes, epitools, maps, clipr,
               ggmap, twitteR, ROAuth, tm, rtweet, base64enc,
               httpuv, SnowballC, RColorBrewer, wordcloud,
               ggwordcloud, tidyverse, boot)
#+end_src


In order to access data through the /Twitter/ API it is necessary to use tokens provided through a /Twitter/ developer account, these can be loaded into =rtweet= using the =create_token()= function as shown in listing [[tkn]].

#+NAME: tkn
#+CAPTION: Import the twitter tokens (redacted)
#+begin_src R
# Set up Tokens ===========================================================

options(RCurlOptions = list(
  verbose = FALSE,
  capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"),
  ssl.verifypeer = FALSE
))

setup_twitter_oauth(
  consumer_key = "*************************",
  consumer_secret = "**************************************************",
  access_token = "**************************************************",
  access_secret = "*********************************************"
)

# rtweet ==================================================================
tk <-    rtweet::create_token(
  app = "SWA",
  consumer_key    = "*************************",
  consumer_secret = "**************************************************",
  access_token    = "**************************************************",
  access_secret   = "*********************************************",
  set_renv        = FALSE
#+end_src

Using the =rtweet= library 1000 tweets containing a mention of /Ubisoft/, in
reverse-chronological order, can be returned and then saved to disk using the
=save()= function as shown in listing [[save]]. [fn:query]

#+NAME: save
#+CAPTION: Save the Tweets to the HDD as an ~rdata~ file
#+begin_src R
 n <- 1000
 tweets.company <- search_tweets(q = 'ubisoft', n = n, token = tk,
                                 include_rts = FALSE)
 save(tweets.company[,], file = "resources/Download_1.Rdata")
#+end_src

** 8.2.2 Count of Followers and Friends
To identify the number of users that are contained in the /tweets/ it
is necessary to distinguish which of the usernames are unique, this can be
achieved by using either the =unique()= or =duplicated()= functions. The
=unique= function can be used to return a vector of unique elements which can be
passed to the =length()= function in order to return the number of unique users,
this is demonstrated in the listing [[count]] and provides that 81.7% of the tweets were
made by unique users.


The =duplicated= function can be used to identify and omit duplicated observations, this is also shown in the listing [[count]].

#+NAME: count
#+CAPTION: Return follower count of twitter posts
#+begin_src R
(users <- unique(tweets.company$name)) %>% length()
x <- tweets.company$followers_count[!duplicated(tweets.company$name)]
y <- tweets.company$friends_count[!duplicated(tweets.company$name)]

## > [1] 817
#+end_src


** 8.1.3 Summary Statistics
The average number of friends and followers from users who posted tweets mentioning /Ubisoft/ can be returned using the ~mean()~ as shown in listing [[mean]]
this provides that on average each user has 586 friends and 63,620 followers.

#+NAME: mean
#+CAPTION: Determine the average number of friends and followers
#+begin_src R
x<- rnorm(090)
y<- rnorm(090)
(xbar <- mean(x))
(ybar <- mean(y))

## > [1] 4295.195
## > [1] 435.9449
#+end_src

** 8.1.4 Above Average Followers
Each user can be compared to the average number of followers, by using a logical
operator on the vector (e.g. ~y > ybar~), this will return an output of logical
values. */R/* will coerce logical into 1/0 values meaning that the mean value
will return the proportion of =TRUE= responses as shown in listing [[pyhat]]. This
provides that:

+ 2.4%  of the have identified have an above-average *number of followers*.
+ 20.6% of the users identified have an above-average *number of friends*.

#+NAME: pyhat
#+CAPTION: Calculate the proportion of users with above average follower counts
#+begin_src R
(px_hat <- mean(x>xbar))
(py_hat <- mean(y>ybar))

## > [1] 0.0244798
## > [1] 0.2729498
#+end_src


** 8.1.5 Bootstrap confidence intervals
*** a/b.) Generate a bootstrap distribution

A bootstrap assumes that the population is an infinitely large repetition of the
sample and maybe produced concerning follower counts by resampling with
replacement/repetition, this can then be plotted using the =ggplot2= library as demonstrated
in listings [[btpop]] and [[btgg]] and shown in figure [[btpopfg]].

This shows that the population follower counts is a non-normal skew-right
distribution, which is expected because the number of friends is an integer value bound by zero cite:nist2013.

#+NAME: btpop
#+CAPTION: Bootstrapping a population from the sample.
#+begin_src R
## Resample the Data
(bt_pop <- sample(x, size = 10^6, replace = TRUE)) %>% head()

## > [1]   7 515 262 309 186 166
#+end_src

#+NAME: btgg
#+CAPTION: Use ggplot2 to Produce Histogram of Follower Counts Bootstrapped from the population
#+begin_src r
## Make the Population
bt_pop_data <- tibble("Followers" = bt_pop)
ggplot(data = bt_pop_data, aes(x = Followers)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 35, col = "pink") +
  geom_density(col = "violetred2") +
  scale_x_continuous(limits = c(1, 800)) +
  theme_bw() +
  labs(x = "Number of Followers", y = "Density",
       title = "Bootstrapped population of Follower Numbers")

#+end_src

#+attr_html: :width 400px
#+attr_latex: :width 12cm
#+NAME: btpopfg
#+CAPTION: Histogram of the bootsrapped population of follower counts
[[./Figures/BootStrap_Pop.png]]

*** c.) Estimate a Confidence Interval for the population mean Follower Counts
To perform a bootstrap for the population mean value of follower counts it is necessary to:

1. Resample the data with replacement
   + i.e. randomly select values from the sample allowing for repetition
2. Measure the statistic of concern
3. Replicate this a sufficient number of times
   + i.e. Greater than or equal to 1000 times [[cite:davison1997][Ch. 5]]

This is equivalent to drawing a sample from a population that is infinitely large and constructed of repetitions of the sample. This can be performed in */R/* as shown in listing [[bt1s]].


#+NAME: bt1s
#+CAPTION: Confidence Interval of Mean Follower Count in Population
#+begin_src R
xbar_boot_loop <- replicate(10^3, {
  s <- sample(x, replace = TRUE)
  mean(s)
  })
quantile(xbar_boot_loop, c((1-0.97)/2, (1+0.97)/2))

##       1.5%      98.5%
##   588.4189 10228.7352
#+end_src

A 97% probability interval is such that a sample drawn from a population will contain the population mean in that interval 97% of the time, this means that it may be concluded with a high degree of certainty that the true population mean lies between 588 and 10228.

**** Alternative Approaches
If this data was normally distributed it may have been appropriate to consider
bootstrapping the standard error and using a $t$ distribution, however it is more appropriate to use a
percentile interval for skewed data such as this, in saying that however this method is not considered to be very accurate in the literature and is often too narrow. [[cite:hesterberg2015][Section 4.1]]

- It's worth noting that the normal $t$ value bootstrap offers no advantage over
  using a $t$ distribution (other than being illustrative of bootstrapping
  generally) [[cite:hesterberg2015][Section 4.1]]


  The =boot= package is a bootstrapping library common among authors in the data science sphere
  [[cite:james2013][p. 295]] [[cite:wiley2019][p. 237]] that implements
  confidence intervals consistent with work by Davison and Hinkley
  cite:ripley2020 in their textbook /Bootstrap Methods and their Application/.
 In this work it is provided that the $BC_{a}$ method of constructing confidence
  intervals is  superior to mere percentile
  methods in terms of accuracy [[cite:davison1997][Ch. 5]], a sentiment echoed in the literature. [[cite:carpenter2000,davison1997][Ch. 5]]

 Such methods can be implemented in */R/* by passing a function to the =boot= call as shown in listing [[bootx]]. This provides a broader interval, suggesting that the true confidence interval could lie between 1079 and 16227 followers.

 #+NAME: bootx
 #+CAPTION: Bootstrap of population mean follower count implementing the $BC_{a}$ method
 #+begin_src r
xbar_boot <- boot(data = x, statistic = mean_val, R = 10^3)
boot.ci(xbar_boot, conf = 0.97, type = "bca", index = 1)

## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = xbar_boot, conf = 0.97, type = "bca", index = 1)
##
## Intervals :
## Level       BCa
## 97%   ( 1079, 16227 )
## Calculations and Intervals on Original Scale
## Warning : BCa Intervals used Extreme Quantiles
## Some BCa intervals may be unstable
## Warning message:
## In norm.inter(t, adj.alpha) : extreme order statistics used as endpoints
 #+end_src


*** d.) Estimate a Confidence Interval for the population mean Friend Counts
A Confidence interval for the population mean friend counts may be constructed
in a likewise fashion as shown in listings [[booty]]. This provides that the 97%
confidence interval for the /population mean friend count/ is between 384 and 502
(or 387 and 496 if the $BC_{a}$ method used, they're quite close and so the more
conservative percentile method will be accepted).

#+NAME: booty
#+CAPTION: Bootstrap of population mean follower count
#+begin_src R
# d.) Estimate a Confidence Interval for the populattion mean Friend Count ===
# Using a Percentile Method #####################################################
ybar_boot_loop <- replicate(10^3, {
  s <- sample(y, replace = TRUE)
  mean(s)
  })
quantile(ybar_boot_loop, c(0.015, 0.985)

# Using BCA Method #############################################################
mean_val <- function(data, index) {
  X = data[index]
  return(mean(X))
}

xbar_boot <- boot(data = y, statistic = mean_val, R = 10^3)
boot.ci(xbar_boot, conf = 0.97, type = "bca", index = 1)


##     1.5%    98.5%
## 383.7619 501.5903
##
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = xbar_boot, conf = 0.97, type = "bca", index = 1)
##
## Intervals :
## Level       BCa
## 97%   (386.8, 496.7 )
## Calculations and Intervals on Original Scale
## Some BCa intervals may be unstable
#+end_src

** FIXME 8.1.6 Estimate a 97% Confidence Interval for the High Friend Count Proportion
:PROPERTIES:
:DIR:      /home/ryan/Dropbox/Notes/DataSci/Social_Web_Analytics/SWA-Project/docs/
:END:
In order to bootstrap a confidence interval for the proportion of users with
above average follower counts, repeatedly draw random samples from an infinitely
large population composed entirely of the sample, and record the sampled
proportion. This can be achieved by resampling the observations of above and
below as shown in the listing [[phat]].

This provides that:
 + The 97% confidence interval for the population proportion of users that have
   an above-average number of friends is between 0.24 and 0.31.
  + i.e. The probability, of any given sample taken from the population,
    containing the population mean within this interval, would be 97%; that does
    not, however, imply that there is a 97% probability that this interval
    contains the value.

   #+begin_comment
   i.e. 97% of samples drawn from a population will have the population mean
   inside the corresponding confidence interval, So if we intended to sample a
   population and draw a confidence interval, that confidence interval would be
   the probability of any given sample containing the population mean.

   Given a sample however, it is not correct to then conclude that the confidence
   interval is the probability of that sample containing the value because we
   simply have no information on how good the sample is at predicting the
   population, it could be a biased sample and we can not know what
   \mu is anyway.

   All we know is that if many samples were drawn and one randomly selected, the
   probability of that sample containing the population in the interval would be
   97%

   See [[../docs/ConfIntNotes.pdf][This Attachment]]
   #+end_comment

#+NAME: phat
#+CAPTION: Bootstrap of Proportion of Friends above average
#+begin_src R
# 8.1.6 High Friend Count Proportion -------------------------------------------
prop <- factor(c("Below", "Above"))
## 1 is above average, 2 is below
py_hat_bt <- replicate(10^3, {
  rs      <- sample(c("Below", "Above"),
                    size = length(y),
                    prob = c(py_hat, 1-py_hat),
                    replace = TRUE)
isabove <- rs == "Above"
mean(isabove)
})
quantile(py_hat_bt, c(0.015, 0.985))


##      1.5%     98.5%
## 0.2399021 0.3072215
## > > > . + > > >
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
##
## CALL :
## boot.ci(boot.out = py_hat_boot, conf = 0.97, type = "bca")
##
## Intervals :
## Level       BCa
## 97%   ( 0.2399,  0.3072 )
## Calculations and Intervals on Original Scale
#+end_src
** 8.1.7 Is the Number of Friends Independent to the Number of Followers
One method to determine whether or not the number of followers is independent of the number of friends is to bin the counts and determine whether or not the distribution of users across those counts is consistent with the hypothesis of independence.

*** Bin the Follower and Friend Categories
The counts may be binned by performing a logical interval test as shown in listing [[7bin]].

#+NAME: 7bin
#+CAPTION: Use Logical Test to Assign observations into bins
#+begin_src R
## Assign Categories
x_df <- data.frame(x)
x_df$cat[0       <= x_df$x & x_df$x < 100] <- "Tens"
x_df$cat[100     <= x_df$x & x_df$x < 1000] <- "Hundreds"
x_df$cat[1000    <= x_df$x & x_df$x < 2000] <- "1Thousands"
x_df$cat[2000    <= x_df$x & x_df$x < 3000] <- "2Thousands"
x_df$cat[3000    <= x_df$x & x_df$x < 4000] <- "3Thousands"
x_df$cat[4000    <= x_df$x & x_df$x < 5000] <- "4Thousands"
x_df$cat[5000    <= x_df$x & x_df$x < Inf] <- "5ThousandOrMore"

### Make a factor
x_df$cat <- factor(x_df$cat, levels = var_levels, ordered = TRUE)

### Determine Frequencies
(x_freq <- table(x_df$cat) %>% as.matrix())

## ** b) Find the Friend Count Frequency ===========================================
## Assign Categories
y_df <- data.frame(y)
y_df$cat[0       <= y_df$y & y_df$y < 100] <- "Tens"
y_df$cat[100     <= y_df$y & y_df$y < 1000] <- "Hundreds"
y_df$cat[1000    <= y_df$y & y_df$y < 2000] <- "1Thousands"
y_df$cat[2000    <= y_df$y & y_df$y < 3000] <- "2Thousands"
y_df$cat[3000    <= y_df$y & y_df$y < 4000] <- "3Thousands"
y_df$cat[4000    <= y_df$y & y_df$y < 5000] <- "4Thousands"
y_df$cat[5000    <= y_df$y & y_df$y < Inf]  <- "5ThousandOrMore"

### Make a factor
y_df$cat <- factor(y_df$cat, levels = var_levels, ordered = TRUE)

### Determine Frequencies
(y_freq <- table(y_df$cat) %>% as.matrix())
#+end_src

*** Find the Group frequency
These values may be tabulated to count the occurrence of users among these categories as shown in listing [[7fr]] and table [[7frt]].

#+NAME: 7fr
#+CAPTION: Tabulate the binned counts for the distribution of users among amount and status.
#+begin_src R
vals <- t(cbind(x_freq, y_freq))
rownames(vals) <- c("Followers.x", "followers.y")
vals

##             Tens Hundreds 1Thousands 2Thousands 3Thousands 4Thousands
## Followers.x  421      317         39         11          9          2
## followers.y  262      476         47         15          6          9
##             5ThousandOrMore
## Followers.x              18
## followers.y               2
#+end_src

#+NAME: 7frt
#+CAPTION: Binned Friend and Follower counts, transposed relative to code.
|                      | **/Followers/** | **/Friends/** |
| /Tens/               |             421 |           262 |
| /Hundreds/           |             317 |           476 |
| /1 - Thousands/      |              39 |            47 |
| /2 - Thousands/      |              11 |            15 |
| /3 - Thousands/      |               9 |             6 |
| /4 - Thousands/      |               2 |             9 |
| /5 Thousand or More/ |              18 |             2 |

*** Find the Expected Counts under each group and test for independence
The expected count of each cell, under the assumption that the two metrics are
independent will be the proportion of users per bracket multiplied by the number
of users in that status group. This implies that any cell will be:

+ the product of the row sum, multiplied by the column sum divided by the number of counts.

This can be equivalently expressed as an outer product as shown in equation
[[eqref:eq:1]], in */R/* this operation is denoted by the =%o%= operator, which is
shorthand for the =outer()= function, this and other summary statistics may be
evaluated as shown in the listing [[smst]].

The outer product is such that:


$$
\mathbf{u} \otimes \mathbf {v} =\mathbf {u} \mathbf {v} ^{\textsf {T}}={\begin{bmatrix}u_{1}\\u_{2}\\u_{3}\\u_{4}\end{bmatrix}}{\begin{bmatrix}v_{1}&v_{2}&v_{3}\end{bmatrix}}={\begin{bmatrix}u_{1}v_{1}&u_{1}v_{2}&u_{1}v_{3}\\u_{2}v_{1}&u_{2}v_{2}&u_{2}v_{3}\\u_{3}v_{1}&u_{3}v_{2}&u_{3}v_{3}\\u_{4}v_{1}&u_{4}v_{2}&u_{4}v_{3}\end{bmatrix}}.
$$

This means the matrix of expected frequencies can be expressed as an outer product thusly:

\begin{align}
\mathbf{\vec{e}}= \frac{1}{n} \times \begin{bmatrix} \sum^{n}_{j= 1} \left[ o_{1j} \right] \\  \sum^{n}_{j= 1}
\left[ o_{2j} \right]  \\ \sum^{n}_{j= 1} \left[ o_{3j} \right]   \\
\sum^{n}_{j= 1} \left[ o_{4j} \right]  \\ \vdots  \\
\sum^{n}_{j= 1} \left[ o_{nj} \right]     \end{bmatrix}
\begin{bmatrix}  \sum^{n}_{j= 1} \left[ o_{i1}  \right] \\  \sum^{n}_{j= 1}
\left[ o_{i2}  \right] \\ \sum^{n}_{j= 1} \left[ o_{i3}  \right] \\ \cdots \\
\sum^{n}_{j= 1} \left[ o_{in}  \right]   \end{bmatrix}  ^{\mathrm{T}} \label{eq:1}
\end{align}

#+NAME: smst
#+CAPTION: Calculate Expected frequency of values under the assumption of independence.
#+begin_src R
## ***** Calculate Summary Stats
n <- sum(vals)
bracket_prop <- colSums(vals) / n
metric_prop  <- rowSums(vals) / n
o <- vals
e <- rowSums(vals) %o% colSums(vals) / n
chi_obs <- sum((e-o)^2/e)
#+end_src

**** *Testing Independence*
\newline
To test whether or not the distribution of users among brackets is
independent of being a follower or friend a $\chi^{2}$ test may be used, this
can be evaluated from a model or simulated, in */R/*, the simulated test is
shown in the listing [[chib]], this provides a $p$ -value < 0.0005, which means that the hypothesis of independence may be rejected with a high degree of certainty.

#+NAME: chib
#+CAPTION: Chi-Square testing for independence between friend and follower bin categories.
#+begin_src R
chisq.test(vals, simulate.p.value = TRUE)


## Pearson's Chi-squared test with simulated p-value (based on 2000
## replicates)
##
## data:  vals
## X-squared = 88.109, df = NA, p-value = 0.0004998
#+end_src

***** *From First Principles*
\newline
The $\chi^{2}$ statistic may be performed from first principles by randomly
sampling the values at the rate at which they occurred, tabulating those counts, measuring the $\chi^{2}$ -value, and then repeating this many times.

Because the samples are random they must be independent and average number of
positives is hence an estimate for the /FPR/, which is, in turn, an estimate for
the $p$ -value. This technique is demonstrated in the listing [[chif]], the p-value
being returned as 0.0004, this value is consistent with the value produced by
*/R/*'s built-in =chisq.test= function and so is accepted.

#+NAME: chif
#+CAPTION: Performing a $\chi^{2}$ statistic from first principles
#+begin_src R
## ***** Create Vectors of factor levels
brackets <- unique(x_df$cat)
metrics <- c("follower", "friend")

## ***** Simulate the data Assuming H_0
## I.e. assuming that the null hypothesis is true in that
## the brackets assigned to followers are independent of the friends
## (this is a symmetric relation)

s <- replicate(10^4,{
  ## Sample the set of Metrics
  m <- sample(metrics, size = n, replace = TRUE, prob = metric_prop)

  ## Sample the set of Brackets (i.e. which performance bracket the user falls in)
  b <- sample(brackets, size = n, replace = TRUE, prob = bracket_prop)

  ## Make a table of results
  o <- table(m, b)
  o

  ## Find What the expected value would be
  e_sim <- t(colSums(e) %o% rowSums(e) / n)

  ## Calculate the Chi Stat
  chi_sim <- sum((e_sim-o)^2/e_sim)
  chi_sim

  ## Is this more extreme, i.e. would we reject the null hypothesis?
  chi_sim > chi_obs

})

mean(s)

## [1] 4e-04
#+end_src
*** Conclusion
The $p$ -value measures the probability of rejecting the null hypothesis when it
is true, i.e. the probability of a detecting a /false positive/, a very small
$p$ -value is hence good evidence that the null hypothesis should be rejected.

In saying that however the $p$ -value is distinct from the /power/ statistic,
which is a measure of /the probability of accepting the alternative hypothesis/
when it is true, a low $p$ -value is not a measurement of the probability of
being correct.

Nonetheless, may we conclude, with a high degree of certainty, that the follower and
friend counts are not independent of one another.
* 8.2 Finding Themes in tweets
** 8.2.8 Find Users with Above Average Friend Counts
Users with Above average Friend Counts can be identified by filtering the tweets
data frame for two conditions:

1. non-duplicated =user-id=
2. =friend_count= greater than average

This can be achieved easily using the =dplyr= package as shown in the listing [[hfdp]], the top 20 of these users are shown in table [[hfls]] of the appendix

#+NAME: hfdp
#+CAPTION: Use =dplyr= to Filter for Users with a high Friend Count
#+begin_src R
select <- dplyr::select
filter <- dplyr::filter
interested_vars <- c("user_id", "friends_count")
(friend_counts <- tweets.company %>%
  select(interested_vars) %>%
  filter(!duplicated(user_id)))

(high_friends <- friend_counts %>%
  filter(friends_count > mean(friends_count, na.rm = TRUE)))

## Export Friends List
write.csv(high_friends[order(
  high_friends$friends_count,
  decreasing = TRUE),], file = "/tmp/highfriend.csv")
#+end_src

** 8.2.9 Find Users with Below Average Friend Counts
Users with high friends may be determined by a similar method (or by taking the complement of the high friends) as shown in the listing [[lfcd]], the lowest 20 of these users are shown in table [[lftb]] of the appendix.

#+NAME: lfcd
#+CAPTION: Use =dplyr= to Filter for Users with a low Friend Count
#+begin_src R
(low_friends <- friend_counts %>%
  filter(friends_count <= mean(friends_count, na.rm = TRUE)))

 low_friends <- low_friends[order(
   low_friends$friends_count,
   decreasing = TRUE),]

## Export Users
write.csv(low_friends[order(
  low_friends$friends_count,
  decreasing = FALSE),], file = "/tmp/lowfriend.csv")
#+end_src

** 8.2.10 Find the /Tweets/ corresponding to users with high or low friend counts
The tweets corresponding to users with high and low friend counts can be
identified by filtering the data frame based on the friend count and using that
to index the tweets from the data frame [fn:1], alternatively, it is possible
to test whether or not the ID of a user appears in the high or low vector
set using the =%in%= operator as shown in the listing [[8210]].

#+NAME: 8210
#+CAPTION: Identify tweets corresponding to users with high and low friend counts
#+begin_src R
## Method 1
friend_test <- tweets.company$friends_count > mean(tweets.company$friends_count)
tweets_high <- tweets.company$text[friend_test]
tweets_low <- tweets.company$text[!friend_test]

## Method 2                                                                 :15b5a74:
tweets_high <- tweets.company$text[tweets.company$user_id %in%  high_friends$user_id]
tweets_low  <- tweets.company$text[tweets.company$user_id %in%  low_friends$user_id]
tweets <- c(tweets_high, tweets_low)

## Mark as High or Low
tweets_low <- cbind(tweets_low, rep("Low_Friend", length(tweets_low)))
tweets_high <- cbind(tweets_high, rep("High_Friend", length(tweets_high)))
tweets <- as.data.frame(rbind(tweets_high, tweets_low))
tweets$Friend_Status  <- factor(tweets$Friend_Status)
#+end_src

** 8.2.11 Clean the tweets
*** Create a Corpus Object
To clean the tweets it is necessary to create a corpus object as shown in the listing [[cpmk]], it is possible to pass a dataframe source to include the user ID, this isn't strictly necessary however because the =tm= package preserves order when performing transformations.

#+NAME: cpmk
#+CAPTION: Create a Corpus from the tweets
#+begin_src R
tweet_source <- tm::VectorSource(tweets$text)
tweet_corpus <- tm::Corpus(x = tweet_source)
#+end_src

Next, it is necessary to choose an encoding, a primary consideration of this is whether or not the use of /emoji/ characters will influence the model performance. There is research to suggest that emoji can be used as predictive features cite:lecompte2017 and that they can improve /sentiment analysis/ models cite:shiha2017 that implement a /bag of words/ approach. For these reasons /emoji/ characters will be preserved and [[http://www.utf-8.com/][UTF-8]] implemented.

In order to encode the data as /UTF-8/, the =iconv= function can be used as shown in listing [[icv]].

#+NAME: icv
#+CAPTION: Encode the Data as UTF-8
#+begin_src R
encode <- function(x) {
    iconv(x, to = "UTF-8")
#    iconv(x, to = "latin1")
  #  iconv(x, to = "ASCII")
}

tweet_corpus <- tm_map(x = tweet_corpus, FUN = encode)
tweet_corpus_raw <- tweet_corpus
#+end_src

*** Process the tweets
Before analysis the tweets should be modified to remove characters that may interfere with categorizing words, this is referred to as cleaning, in particular, the following should be implemented:

1. Remove URL's
2. Remove Usernames
3. Remove numbers
4. Remove punctuation
5. Remove whitespace
6. Case fold all characters to lower case
7. Remove a set of stop words
8. Reduce each word to its stem

In particular, it is important to reduce words to lower case before removing stop words otherwise an unorthodox use of capitalization may prevent the word from being removed throughout.

The stop word =ubisoft= will also be used, this was the query term so it's expected to turn up at a very high frequency, the words =can= and ='s= also occurred quite frequently and so were removed.

The cleaning can be implemented by mapping functions over the corpus, which is fundamentally a list, this can be performed via the =tm_map= function as shown in the listing [[cltw]].



#+NAME: cltw
#+CAPTION: Use the =tm_map= function to clean the tweets
#+begin_src R
mystop <- c(stopwords(), "’s", "can", "ubisoft", "@ubisoft", "#ubisoft")# <<stphere>>

clean_corp <- function(corpus) {
  ## Remove URL's
  corpus <- tm_map(corpus,content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+","",x)))
  ## Remove Usernames
  corpus <- tm_map(corpus,content_transformer(function(x) gsub("@\\w+","",x)))
  ## Misc
  corpus <- tm_map(corpus, FUN = removeNumbers)
  corpus <- tm_map(corpus, FUN = removePunctuation)
  corpus <- tm_map(corpus, FUN = stripWhitespace)
  corpus <- tm_map(corpus, FUN = tolower)
  corpus <- tm_map(corpus, FUN = removeWords, mystop)
  ## stopwords() returns characters and is fead as second argument
  corpus <- tm_map(corpus, FUN = stemDocument)
  return(corpus)
}

tweet_corpus_clean <- clean_corp(tweet_corpus)
#+end_src

** 8.2.12 Display the first two tweets before/after processing
The tweets can be viewed from inside the corpus by selecting with the =[= function [fn:2] as demonstrated in listing [[ispt]], the first /tweet/ was rendered empty by the processing and the following two tweets were:

+ */Pre-Processing/*
    #+begin_quote
   + "Today was the first time in over a month that I have gone 24 hours without
    checking the coronavirus death toll. Thanks =@Ubisoft=."

   + "@btwimskrank =@TheDivisionGame= =@UbiMassive= =@Ubisoft= =@jgerighty= =@hamishbode=
    =@Tideman92= =@janeyo_jane= =@slimjd= Very odd... I'll even post a video about it."
    #+end_quote
+ */Post-Processing/*
    #+begin_quote
    + "today first time month gone hour without check coronavirus death toll thank"

    + "odd ill even post video"
    #+end_quote

#+NAME: ispt
#+CAPTION: Display the tweets before and after cleaning
#+begin_src R
tweet_corpus_raw[[1]]$content
tweet_corpus_clean[[1]]$content
tweet_corpus_raw[[2]]$content
tweet_corpus_clean[[2]]$content
tweet_corpus_raw[[3]]$content
tweet_corpus_clean[[3]]$content


## [1] NA
## [1] "NA"
## [1] "Today was the first time in over a month that I have gone 24 hours without checking the coronavirus death toll. Thanks @Ubisoft."
## [1] "today first time month gone hour without check coronavirus death toll thank"
## [1] "@btwimskrank @TheDivisionGame @UbiMassive @Ubisoft @jgerighty @hamishbode @Tideman92 @janeyo_jane @slimjd Very odd... I'll even post a video about it."
## [1] "odd ill even post video"
#+end_src


** 8.2.13 Create a Term Document Matrix
*** Apply Weighting Manually
**** *Create Term Documen Matrix*
\newline
A term Document matrix (and it's transpose) can be constructed from a =corpus= using the
=tm:TermDocumentMatrix= function as shown in listing [[tdm]].

#+NAME: tdm
#+CAPTION: Create the /Term Document Matrix/ and it's transpose using built-in functions.
#+begin_src R
tweet_matrix_tdm <- TermDocumentMatrix(tweet_corpus_clean)
tweet_matrix_dtm <- DocumentTermMatrix(tweet_corpus_clean)
#+end_src

**** *Apply TF-IDF Weighting*
\newline
Weighted term frequency is defined as shown in equation eqref:tfidf, where:

+ $f_{d,t}$ is the frequency of a given term $t$ in the a document $d$
+ $w_{d,t}$ is the weight of a given term $t$ in the a document $d$
+ $N$ is the number of documents
+ $f_{t}$ is the number of documents containing $t$

\begin{equation}\begin{aligned}
w_{d, t} &=\mathrm{TF}_{t} \times \mathrm{IDF}_{d, t} \\
&=\log _{e}\left(f_{d, t}+1\right) \log _{e}\left(\frac{N}{f_{t}}\right) \label{tfidf}
\end{aligned}\end{equation}

This would require multiplying each term of each row of the $\mathrm{TF}_{t}$ matrix by the corresponding vector element of $\mathrm{IDF}_{d,t}$, this can be implemented by taking the matrix product of a diagonalized matrix, this is shown in listing [[mytfidf]].

#+NAME: mytfidf
#+CAPTION: Apply TF-IDF Weigting
#+begin_src R
N <- nrow(as.matrix(tweet_matrix_dtm))   # Number of Documents
ft=colSums(as.matrix(tweet_matrix_dtm) > 0) #in how many documents term t appeared in,

TF <- log(as.matrix(tweet_matrix_dtm) + 1)  # built in uses log2()
IDF <- log(N/ft)

tweet_weighted           <- TF %*% diag(IDF)
colnames(tweet_weighted) <- colnames(tweet_matrix_dtm)
#+end_src

There is however a function built into the =tm= package that will weight term-document matrices and this will instead be implemented to analyze the data because it will produce more maintainable code.

*** Apply Weighting with Built-in Function
In order to create a term document matrix (and its transpose) with TF-IDF weighted values, the =weighting= argument may be specified as =weightTfIdf= by passing an appropriate list to the =control= argument of the =TermDocumentMatrix= function, as shown in listing [[btdm]]

#+NAME:btdm
#+CAPTION: Create a Document Term Matrix by transforming a Term Document Matrix
#+begin_src R
tweet_weighted_tdm <- tm::TermDocumentMatrix(x = tweet_corpus_clean, control = list(weighting = weightTfIdf))
tweet_weighted_dtm <- as.DocumentTermMatrix()  %>%
                        as.matrix()
#+end_src

*** Remove Empty Documents
Empty Documents may be removed from the matrix by a logical test as shown in the listing  [[emp]] [fn:3] this provides that 328 documents were empty following the processing. A summary of the first rows and columns of this matrix, following the removal of empty documents, is provided in table [[tfidf]] of the appendix.

#+NAME:emp
#+CAPTION: Remove empty documents from the Matrix, observe that the =[-null]= filter is wrapped in an =if= statement to prevent an empty vector bug.
#+begin_src R
null = which(rowSums(as.matrix(tweet_weighted_dtm)) == 0)
rowSums(as.matrix(tweet_weighted_dtm)==0)

if(length(null)!=0){
  tweet_weighted_dtm = tweet_weighted_dtm[-null,]
}

length(null)

## [1] 328
#+end_src
** 8.2.14 How many Clusters are there
*** Use Cosine Distance
To consider clustering, it can be more effective to consider the distance between the weighted documents in terms of cosine distance, the cosine distance can be calculated from the euclidean distance using the identity shown in eqref:cos, and this can be implemented in */R/* by taking the matrix product of a diagonalized matrix as shown in listing [[dist]].

\begin{align}
\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)&= \left| \left| \mathbf{X}-\mathbf{Y} \right| \right| \\
&= \sqrt{\sum^{n}_{i= 1}   \left[ \left( x_i-y_i \right)^2 \right] } \\
\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)^2&= \sum^{n}_{i= 1}  \left[ \left( x_i-y_i \right)^2 \right] \\
&= \sum^{n}_{i= 1}   \left( x^2 \right)+  \sum^{n}_{i= 1}   \left( y_i^2 \right)+ 2 \sum^{n}_{i= 1}   \left( x_iy_i \right) \\
&= 1+ 1 +  2 \times  \frac{\sum^{n}_{i= 1}   \left( x_iy_i \right)}{\left( 1 \right) }\\
&= 2+ 2\times \frac{\sum^{n}_{i= 1}   \left( x_iy_i \right)}{\left| \left| \mathbf{X} \right| \right|\times \left| \left| \mathbf{Y} \right| \right|}\\
&= 2+ 2 \cos\left( \mathbf{X}, \mathbf{Y} \right)\\
\ \\
& \implies  \left( 1- \cos\left( \mathbf{X}, \mathbf{Y} \right) \right) = \frac{\mathrm{dist}\left( \mathbf{X}, \mathbf{Y} \right)}{2} \label{cos}
\end{align}

#+NAME: dist
#+CAPTION: Calculate the cosine distance by implementing the identity from equation eqref:cos
#+begin_src R
norm.tweet_weighted_dtm = diag(
                    1/sqrt(rowSums(tweet_weighted_dtm^2))
                    ) %*% tweet_weighted_dtm
D =dist(norm.tweet_weighted_dtm, method = "euclidean")^2/2
#+end_src

*** Project into Euclidean Space
The cosine distance, however, is not an appropriate distance measurement with
which to perform $k$ means clustering and so the data instead should be
projected back into euclidean space, this can be achieved using
/Multi-Dimensional Scaling/ via the =cmdscale= function as shown in listing [[mds]].

The distance is a measure of between document distance so the number of
dimensions should correspond to the number of documents, however, if there are
zero-value eigenvalues, these dimensions won't help explain the data in the
projection, hence the number of eigenvalues have been used as the dimension of
projection in this case.

#+NAME: mds
#+CAPTION: Project the /DocumentTermMatrix/ into /Euclidean Space/, identifying the number of dimensions from the number of non-zero eigen vectors.
#+begin_src R
l <- min(nrow(tweet_weighted_dtm),
         ncol(tweet_weighted_dtm))
ev <- eigen(tweet_weighted_dtm[1:l, 1:l])
k <- (ev$values != 0) %>% sum()

mds.tweet_weighted_dtm <- cmdscale(D, k=k) #TODO What should K be? see issue #10
#+end_src

*** Measure Within Cluster Variance
To determine the appropriate number of clusters, the within cluster
variance can be measured, the number of clusters at which this value ceases to
decrease is indicative of a potentially appropriate number of clusters. This is
implemented in listing [[ssw]] and shown in figure [[sswp]].

#+NAME: ssw
#+CAPTION: Use a loop to evaluate the performace of various cluster models, plot this with /ggplot2/
#+begin_src R
set.seed(271)
n = 15 # Assume it bends at 7 clusters
SSW = rep(0, n)
for (a in 1:n) {
  K = kmeans(mds.tweet_weighted_dtm, a, nstart = 20)
  SSW[a] = K$tot.withinss
  paste(a*100/n, "%") %>% print()
}
SSW


SSW_tb <- tibble::enframe(SSW)
ggplot(SSW_tb, aes(x = name, y = value)) +
  geom_point(col = "#Cd5b45", size = 5) +
  geom_line(col = "#Da70d6") +
  geom_vline(xintercept = 7, lty  = 3, col = "blue") +
  theme_bw() +
  labs(x = "Number of Clusters",
       y = "Within Cluster Sum of Square Distance",
       title = "Within Cluster Variance across Clusters")
#+end_src

Figure [[sswp]] Indicates a sudden stop of decrease in variance at 7 clusters and following that the within cluster variance begins to decrease at a slightly slower rate. For this reason, 7 could be an appropriate candidate for the number of clusters, however, the minimal amount of change in the within-cluster variance indicates that the data is most likely not clustered at all.


#+NAME: sswp
#+CAPTION: Plot of the Within Cluster Variance of the tweets using cosine distance (projected into Euclidean Space)
#+attr_html: :width 400px
#+attr_latex: :width 10cm
[[./Figures/Q14WithinClusterVariance.png]]

** 8.2.15 Find the Number of tweets in each cluster
Moving forward 3 clusters will be used to categorize the
tweets, 7 is too large and a smaller number will likely be more effective at
categorizing the data (particularly given that the stratification of the data
appears to be quite limited as shown by figure [[sswp]]).

The number of tweets in a cluster may be measured by using the =table= function
as shown in listing [[tab]] and table [[tabo]].

#+NAME: tab
#+CAPTION: The =table= function can count the number of tweets per cluster.
#+begin_src R
K = kmeans(mds.tweet_weighted_dtm, 3, nstart = 20)
table(K$cluster)
#+end_src

#+NAME: tabo
#+CAPTION: Number of tweets in each cluster identified by $k$ means clustering
| Cluster         |  1 |  2 |   3 |
|-----------------+----+----+-----|
| =#= of /tweets/ | 77 | 58 | 537 |

** 8.2.16 Visualise the Clusters
The clusters can be projected into 2D /Euclidean/-Space using /Multi-Dimensional
Scaling/, these dimensions would represent the first two principal components of
the cosine distance of the weighted tweets. This is demonstrated in listings
[[cmds]]. A plot of the friend count-status mapped to shape and clusters mapped to
colour is demonstrated in listing [[pt1c]] shown in [[pt1]].

#+NAME: cmds
#+CAPTION: Use /Multi-Dimensional/ scaling to project the data into 2 dimensions
#+begin_src R
MDS_Euclid_2D <- cmdscale(D, k=2) #TODO What should K be? see issue #10
mds_data$Cluster       <- factor(mds_data$Cluster)

if (nrow(MDS_Euclid_2D[,1:2]) == length(K$cluster)
   && length(K$cluster) == nrow(tweets[-null,])) {
mds_data <- cbind(MDS_Euclid_2D[,1:2], "Cluster" = K$cluster, tweets[-null,])
}

mds_data$Cluster       <- factor(mds_data$Cluster)
names(mds_data)[1:2] <- c("MDS1", "MDS2")
#+end_src

#+NAME: pt1c
#+CAPTION: Create a plot of the distribution of Friends Count Status as shown in figure [[pt1]]
#+begin_src R
ggplot(pca_data, aes(x = MDS1, y = MDS2, col = Cluster)) +
  geom_point(aes(shape = Friend_Status), size = 2) +
  stat_ellipse(level = 0.9) +
  theme_classic() +
  labs(main = "Principal Components of Twitter Data",
       x = TeX("MDS_1"), y = TeX("MDS_2")) +
  scale_shape_discrete(label = c("High Friends", "Low Friends")) +
  guides(shape = guide_legend("Friend Count \n Status"))
#+end_src

#+NAME: pt1
#+CAPTION: Distribution of Friend Count Status over Clusters
#+attr_html: :width 400px
#+attr_latex: :width 9cm
[[./Figures/betclustplot8216.png]]

** 8.2.17 Comment on the Visualisation
Figure [[pt1]] indicates that although there isn't a clear distinction between
clusters there is a separation of the tweets in such a way that does allow some
degree of classification to occur. Figure [[pt1]]  also indicates that having
many or few friends is reasonably independent of the cluster that the tweet
belongs to but that there is possibly also other factors.
** 8.2.18 Cluster with Highest Number Friends
The Number of above or below friends corresponding to a given cluster can be tabulated by using the =table= function as shown in listing [[tfc]] and table [[tfct]], this indicates that there is a difference in the proportion of users with above-average friend counts between clusters with cluster =#1= having the highest proportion of users with above-average friend counts.

#+NAME: tfc
#+CAPTION: Tabulate the distribution of friends in
#+begin_src R
(clust_friend <- table(
  pca_data[,names(pca_data) %in% c("Cluster", "Friend_Status")])
)

cbind(clust_friend,
        "Proportion" = signif(
            (clust_friend[,1] / rowSums(clust_friend)),
            2)
      )
#+end_src

#+NAME: tfct
#+CAPTION: Above Average Friend Counts Across Clusters
| */Cluster/* | */Above Average/* | */Below Average/* | */Proportion/* |
| =1=         |                17 |                27 |           0.39 |
| =2=         |               141 |               407 |           0.26 |
| =3=         |                16 |                64 |           0.20 |

** 8.2.18 Sample Tweets from the cluster
Tweets can be sampled from the clusters by using the =sample= function with a logical test, this can be combined with a =for= loop as shown in the listing [[8218]], this provides the following output.


#+begin_quote
+ *Cluster 1*
  + [[https://twitter.com/search?q=%40BelovedOfBayek&src=typed_query][@BelovedOfBayek]]
     [[https://twitter.com/search?q=%40BayekOfSiwa&src=typed_query][@BayekOfSiwa]] [[https://twitter.com/search?q=%40assassinscreed&src=typed_query][@assassinscreed]] [[https://twitter.com/search?q=%40Ubisoft&src=typed_query][@Ubisoft]] [[https://twitter.com/search?q=%40UbisoftMTL&src=typed_query][@UbisoftMTL]]
    [[https://twitter.com/search?q=%40Captured_Collec&src=typed_query][@Captured\_Collec]] [[https://twitter.com/search?q=%40GamerGram_GG&src=typed_query][@GamerGram\_GG]] [[https://twitter.com/search?q=%40_GameScreenshot&src=typed_query][@\_GameScreenshot]] And those smiles - and
    love - are contagious. Believe me, it makes me very happy! What you have is
    special! 💗 ~ Steffi
  + [[https://twitter.com/search?q=%40Ubisoft][@Ubisoft]] why is it wen I headshot sumone why don’t it register a headshot
    is a 1 shot kill fix your game sir
  + [[https://twitter.com/search?q=%40BikiniBodhi][@BikiniBodhi]] I think we need another movement going. Obviously Ubisoft
    already has plans for the next few releases of ops but we really need an Op
    whose ability is to reinforce more walls than others. Especially another
    castle on the team but for walls. Like 4 walls instead of 2.
  + The United Arab Emirates logged into my Ubisoft account. For why?
  + [[https://twitter.com/search?q=%40Atalagummy][@Atalagummy]] Está gratis este finde en lo de ubisoft creo
+ *Cluster 2*
  + Here's everything you need to know about Ubisoft's Watch Dogs Legion in
   Hindi - Release date, Story - Everything we know about it till now.
   https://t.co/B4lMshJdqw via [[https://twitter.com/search?q=%40YouTube][@YouTube]]
  + [[https://twitter.com/search?q=%40Rainbow6Game][@Rainbow6Game]] [[https://twitter.com/search?q=%40TheGodlyNoob][@TheGodlyNoob]] I have never seen a game company ruin their
   reputation so fast and so careless as Ubisoft
  + tiltei com a ubisoft, dei block na minha conta sem querer, to mt puto, real
  + My first game that really hyped me was a game called rolling thunder back in
   the 80’s https://t.co/mPWim2hwVY
  + [[https://twitter.com/search?q=%40videogamemorals][@videogamemorals]] [[https://twitter.com/search?q=%40PartisanClown][@PartisanClown]] Two more remakes of Lunar: The Silver
   Star, Lunar Legend and Lunar: Silver Star Harmony, were released in 2002
   by Media Rings and Ubisoft and in 2009 by GungHo Online
   Entertainment and Xseed Games, respectively.\
+ *Cluster 3*
  + [[https://twitter.com/search?q=%40Ubisoft][@Ubisoft]] [[https://twitter.com/search?q=%40UbisoftSupport][@UbisoftSupport]] crossplay between xbox and pc for Division 2
    please?
  + [[https://twitter.com/search?q=%40tornado_raphi][@tornado\_raphi]] [[https://twitter.com/search?q=%40Ubisoft][@Ubisoft]] [[https://twitter.com/search?q=%40UbisoftDE][@UbisoftDE]] Haha wollte auch einmal schlau sein
    :(
  + =@Ubisoft= I was in the middle in a game waiting for us to spawn in and it
    took forever and it somehow kicked me from the game for inactivity. Anyway to
    fix this https://t.co/Uz1yZ73R4M
  + [[https://twitter.com/search?q=%40Operatedleech87][@Operatedleech87]] Yo lo veia al revez algo de Ubisoft en Girls Frontline,
    pero igual un juego en consola jalaria.
  + [[https://twitter.com/search?q=%40VGPNetwork][@VGPNetwork]] [[https://twitter.com/search?q=%40GamerGram_GG][@GamerGram\_GG]] [[https://twitter.com/search?q=%40Ubisoft][@Ubisoft]] Haha.. I love that film!
#+end_quote


#+NAME: 8218
#+CAPTION: Sample Tweets from the Individual Clusters
#+begin_src R
set.seed(8923)
  for(i in 1:3) {
    n <- sample(which(pca_data$Cluster == i), size = 5)
    print(tweets$text[n])
    print("===========================")
    print("===========================")
  }
#+end_src

** 8.2.19 Identify themes in the Tweets
*** Highest Friends Count
The themes in the cluster with the highest friend count are:

+ Discussion about upcoming titles
  + For example the /Sunborn-Ubisoft/ collaboration
    + This is where the use of =collab= comes from in figure [[wc1]]
  + The upcoming /Division 2/ release
+ Multiplayer games.
*** Lowest Friends Count

The themes in the cluster with the lowest friends count are:

+ Discussion about older titles
  + e.g. =mincemeat= refers to /Farcry/, mozzi is a /Rainbow 6/ character
  + the recurring use of =remember= and =earliest=
+ Console games and crossplay between them
+ Feature Requests

** 8.2.20 Create WordClouds
Creating a word cloud from the stemmed words will make for a poor visualization
while creating a word cloud based merely on the frequency of words will poorly
identify words central to the theme of the cluster.

In light of this, the most appropriate method to generate word clouds for the
purpose of visualizing the themes of the clusters is to create a document term
matrix based on a corpus cleaned without word stemming as shown in the listing [[wcm0]].
Following that word clouds can then be generated by indexing for the cluster,
this is demonstrated in listing [[wcm]] and shown in figures [[wc1]] and [[wc3]].


#+NAME: wcm0
#+CAPTION: Apply /TF-IDF/ weighting to an unstemmed corpus and then use a =for= loop to create wordclouds corresponding to each cluster.
#+begin_src R
clean_corp_ns <- function(corpus) {
  ## Remove URL's
  corpus <- tm_map(corpus,content_transformer(
    function(x) gsub("(f|ht)tp(s?)://\\S+","",x)))
  ## Remove Usernames
  corpus <- tm_map(corpus,content_transformer(function(x) gsub("@\\w+","",x)))
  ## Misc
  corpus <- tm_map(corpus, FUN = removeNumbers)
  corpus <- tm_map(corpus, FUN = removePunctuation)
  corpus <- tm_map(corpus, FUN = stripWhitespace)
  corpus <- tm_map(corpus, FUN = tolower)
  corpus <- tm_map(corpus, FUN = removeWords, mystop)
  ## stopwords() returns characters and is fead as second argument
  return(corpus)
}

tweet_corpus_clean_ns <- clean_corp(tweet_corpus)

tweet_raw_dtm <- tm::TermDocumentMatrix(x = tweet_corpus_ns,
   control = list(weighting = weightTfIdf)) %>%
  as.DocumentTermMatrix()  %>%
  as.matrix()

null = which(rowSums(as.matrix(tweet_matrix_dtm)) == 0)
length(null)

(rowSums(as.matrix(tweet_matrix_dtm))) %>% table()
if(length(null)!=0){
  tweet_matrix_dtm = tweet_matrix_dtm[-null,]
}

#+end_src

#+NAME: wcm
#+CAPTION: Apply /TF-IDF/ weighting to an unstemmed corpus and then use a =for= loop to create wordclouds corresponding to each cluster.
#+begin_src R
i <- 1

for (i in c(1,3)) {
  n <- which(pca_data$Cluster == i)

  (relevant <- sort(apply(tweet_raw_dtm[n,], 2, mean),
    decreasing = TRUE)[1:30]) %>% head()

  p <- brewer.pal(n = 5, name = "Set2")
    wordcloud(
    words = names(relevant),
    freq = relevant,
    colors = p,
    random.color = FALSE
  )

}
#+end_src

#+NAME: wc1
#+CAPTION: Wordcloud of Cluster =#= 1 using /TF-IDF/ weighting
#+attr_html: :width 400px
#+attr_latex: :width 8cm
[[./Figures/Cluster1Cloud.png]]

#+NAME: wc2
#+CAPTION: Wordcloud of Cluster =#= 2 using /TF-IDF/ weighting
#+attr_html: :width 400px
#+attr_latex: :width 14cm
[[./Figures/Cluster2Cloud.png]]

#+NAME: wc3
#+CAPTION: Wordcloud of Cluster =#= 3 using /TF-IDF/ weighting
#+attr_html: :width 400px
#+attr_latex: :width 10cm
[[./Figures/Cluster3Cloud.png]]

** 8.2.21 Use a Dendrogram to Display the Themes
A Dendrogram can be used to display the themes of the highest and lowest
clusters. To do this the most frequent words of a cluster need to be
filtered out to reduce the dimensions of the dendrogram, following that
a dendrogram may be made to model the cosine distance between the terms. To
achieve this in */R/* the identity from equation eqref:cos can be used and
then a dendrogram modeled over the term document matrix.


*** Highest Friend Count
Any word that occurs more than once was a candidate for the dendrogram visualizing the themes of the cluster with the highest number of friends. This dendrogram is shown in figure [[dn1]] and the corresponding code to produce it is provided in the listing [[dn1c]].

*** Lowest Friend Count
Any word that occurs more than once was a candidate for the dendrogram visualizing the themes of the cluster with the lowest number of friends. This dendrogram is shown in figure [[dn3]] and the corresponding code to produce it is provided in the listing [[dn3c]].

#+NAME: dn1c
#+CAPTION: Create a dendrogram of the terms in the cluster with the highest friends count, average linkage was used.
#+begin_src R
## Filter the Data To match the Cluster
tweet_weighted_dtm_c1 <- tweet_weighted_dtm[pca_data$Cluster==1, ]

## Choose terms of a given frequency to reduce the dimensions
frequent.words = which(colSums(tweet_weighted_dtm_c1 > 0) > 1)
term.matrix = tweet_weighted_dtm_c1[,frequent.words]

## In order to use the Cosine Distance Make each vector have a
## magnitude of 1
unit_term.matrix = term.matrix %*% diag(1/sqrt(colSums(term.matrix^2)))

## Preserve the column Names
colnames(unit_term.matrix) = colnames(term.matrix)
colnames(unit_term.matrix)

## Find the Cosine Distance between the Terms
## (Distance between terms so transpose)
t(unit_term.matrix)
D = dist(t(unit_term.matrix), method = "euclidean")^2/2

## Perform Heirarchical Clustering
h = hclust(D, method="average")
plot(h, main = "Themes of Cluster with Highest Friend Count")






#+end_src

#+NAME: dn3c
#+CAPTION: Create a dendrogram of the terms in the cluster with the highest friends count, average linkage was used.
#+begin_src R
## Filter the Data To match the Cluster
tweet_weighted_dtm_c1 <- tweet_weighted_dtm[pca_data$Cluster==3, ]

## Choose terms of a given frequency to reduce the dimensions
frequent.words = which(colSums(tweet_weighted_dtm_c1 > 0) > 3)
term.matrix = tweet_weighted_dtm_c1[,frequent.words]

## In order to use the Cosine Distance Make each vector have a
## magnitude of 1
unit_term.matrix = term.matrix %*% diag(1/sqrt(colSums(term.matrix^2)))

## Preserve the column Names
colnames(unit_term.matrix) = colnames(term.matrix)
colnames(unit_term.matrix)

## Find the Cosine Distance between the Terms
## (Distance between terms so transpose)
t(unit_term.matrix)
D = dist(t(unit_term.matrix), method = "euclidean")^2/2

## Perform Heirarchical Clustering
h = hclust(D, method="complete")
plot(h, main = "Themes of Cluster with Lowest Friend Count")






#+end_src


#+NAME: dn1
#+CAPTION: Dendrogram of Terms in the cluster with the highest friend count using average Linkage.
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[./Figures/HighDend.png]]


#+NAME: dn3
#+CAPTION: Dendrogram of Terms in the cluster with the highest friend count using average Linkage.
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[./Figures/LowDend.png]]

** 8.2.23 Conclusion
It appears that users who discuss newer titles and upcoming collaborations belong to a cluster with a higher proportion of users with an above average friend count, whereas users that discuss older titles (such as /Assasins Creed/ as demonstrated in figure [[dn3]]) tend to belong to a cluster with a lower proportion of users with high friends count.

This could imply that users discussing new titles are more likely to have more friends or it could imply that Twitter users with a high friend count are more likely to show interest in newer titles and material.
\newpage
* 8.3 Building Networks
** 8.3.24 Find the 10 Most Popular Friends of the Twitter Handle
The =get_friends= function can be used to discover the friends of the [[https://twitter.com/Ubisoft][@ubisoft]]
account, these user /ID/ values can then be passed as an argument to the
=lookup_users= function, this will return the features of those given users and
these results can be sorted by friend count to show the most popular
friends, as shown in the listing [[fdg]], this provides the following list of friends in
order of the number of there popularity:

| 1. =BarackObama=   | 2.  =XboxSupport= | 3. =DEADLINE= | 4.  =gameinformer= | 5.  =Arbys=      |
| 6.  =PeterHollens= | 7.  =HarleyPlays= | 8. =G4C=      | 9.  =Xbox=         | 10. =GailSimone= |


#+NAME: fdg
#+CAPTION: Use =rtweet= to obtain the friends of /Ubisoft/ with the most friends
#+begin_src R
## * 8.2.24 Find 10 Most Popular Friends of the Twitter Handle
## ## ** Get the User ID of Friends of Ubisoft
t <- get_friends("ubisoft", token = tk)
## *** Get More Information of Friends
friends = lookup_users(t$user_id, token = tk)
## **** Inspect the friends
dim(friends)
names(friends)

friends$screen_name[1] #name of friend at index 1
friends$followers_count[1] #examine the follower count of the first friend
friends$screen_name[2]

## ** Find the 10 Most Popular Friends
friendPosition = order(friends$friends_count, decreasing = TRUE)[1:10]
topFriends = friends[friendPosition,] #ids of top 10 friends
## *** Print the top 10 most popular friends
topFriends$screen_name

## [1] "BarackObama"  "XboxSupport"  "DEADLINE"     "gameinformer" "Arbys"
## [6] "PeterHollens" "HarleyPlays"  "G4C"          "Xbox"         "GailSimone"
#+end_src

** TODO 8.3.25 Obtain a *2-degree* egocentric graph centred at /Ubisoft/
*** Download Friends of Friends

The information of friends of friends can be downloaded by using the =get_friends= and =lookup_users=
functions as shown in listing [[gfd]].

#+NAME: gfd
#+CAPTION: Download the information of friends of friends with a loop, an =if= statement is implemented so this is only performed once.
#+begin_src R
if (!file.exists("./AllGraphData.RData")) {
    for (i in 1:10) {
        ## Get friends of Each Friend
        t <- get_friends(topFriends$user_id[i], token <- tk)

        ## Get the Data from Each Friend
        further_friends[[i]] <- lookup_users(t$user_id, token <- tk)

        ## Save the Data
        save(list = ls(), file = "AllGraphData.RData")
    }
} else {
    load(file = "AllGraphData.RData")
}

#+end_src

*** Limit the number of Nodes for Want of Visualisation
 To effectively visualize this graph, the number of nodes will be limited to 13, this can be achieved by filtering the list for more than 13 observations as shown in the listing [[shm]], without performing this the visualization will be dominated by edges and difficult to interpret.

#+NAME: shm
#+CAPTION: Limit the number of nodes in order to aid in visualisation of the graph.
#+begin_src R
n  <- 13
for (a in 1:10) {
  if (nrow(further_friends[[a]]) > n) {
    further_friends[[a]] <- further_friends[[a]][1:n, ]
  }
}
#+end_src

*** Build the Edge List

By building a matrix of screen names a description of the edges of the graph can be built, this is shown in the listing [[bded]].

#+NAME: bded
#+CAPTION: Combine the friends into an Edge matrix.
#+begin_src R
Edges <- cbind(rep(user$screen_name, 10),
           topFriends$screen_name[1:10])  # bind the columns to create a matrix

#+end_src

*** Build the Graph
the =igraph= package can then be used to construct a graph object using the =graph.edgelist= function, this can then be passed to the plot function to produce a plot of the graph as demonstrated in the listing [[mkg]] and shown in figure [[gp1]].

#+NAME: mkg
#+CAPTION: Create a graph from the edges and plot it.
#+begin_src R
g <- graph.edgelist(Edges)
plot(g, layout = layout.fruchterman.reingold,
     vertex.size = 7)
#+end_src


#+NAME: gp1
#+CAPTION: Base Plot of Graph using /Fruchterman-Reingold/ Alorighm
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[./Figures/Graph_Base_1.png]]

*** Use GGPlot2
This Graph could also be produced using =ggplot2= and with an alternate algorithm for arranging the nodes,  to do this a /tidy/ data frame needs to be produced as demonstrated in the listing [[tdy]] and then the ggplot function can be used to build a plot as demonstrated in listing [[ggp]], this produces an alternative visualization of the graph as shown in figure [[ggpt]].

#+NAME: tdy
#+CAPTION: Create a tidy data frame describing the graph in order to use =ggplot2=.
#+begin_src R
## *** Get Edges ###########
Edges <- as.data.frame(Edges)
names(Edges) <- c("Source", "Target")

## *** Build the Data Frame #####
ne <- nrow(Edges) # Number of Edges
laytg <- as.data.frame(igraph::layout.kamada.kawai(g, dim = 2))
laytg$node <- vertex_attr(g)[[1]]
names(laytg)  <- c("xval", "yval", "node")

laytg$xval[laytg$node==Edges$Source[2]]

  ys <- xe <- ye <- xs <- vector(length = ne)
  for (i in seq_len(length(xs))) {
    xs[i] <- laytg$xval[laytg$node==Edges$Source[i]]
    ys[i] <- laytg$yval[laytg$node==Edges$Source[i]]
    xe[i] <- laytg$xval[laytg$node==Edges$Target[i]]
    ye[i] <- laytg$yval[laytg$node==Edges$Target[i]]
  }

  starts    <- data.frame("xval" = xs, "yval" = ys, edgenum = 1:ne) # TODO Make a factor
  ends      <- data.frame("xval" = xe, "yval" = ye, edgenum = 1:ne)
  Edges_val <- as_tibble(rbind(starts,ends))
#+end_src

#+NAME: ggp
#+CAPTION: Use =ggplot2= to construct a visualisation of the graph
#+begin_src R
ggplot(Edges_val, aes(x = xval, y = yval)) +
geom_label_repel(data = laytg,
    mapping = aes(x = xval, y = yval, label = node),
    col = "darkblue", size = 1.5, nudge_x = 0, nudge_y = 0) +
    geom_line(aes(group = edgenum), col = "grey", lty = 2) +
    geom_point(data = laytg,
        aes(x = xval,
                y = yval,
                col = node),
                size = 2) +
    theme_classic() +
  theme(axis.line = element_blank(),
    axis.text.y=element_blank(),axis.ticks=element_blank(),
    axis.text.x=element_blank())
    guides(col = FALSE)
#+end_src

#+NAME: ggpt
#+CAPTION: Graph Visualised using /Kamada-Kawai/ algorithm and built with =ggplot2=
#+attr_html: :width 400px
#+attr_latex: :width 12cm
[[./Figures/ggGraph.png]]
** 8.3.26 Compute the *closeness* centrality score for each user
To compute a closeness centrality score it is necessary to rebuild the
graph as shown in /Question 8.3.25/, it is crucial, however, that the data
not be reduced for meaningful values to be provided, i.e. listing
[[shm]] should be omitted when re-producing the graph object to measure centrality.

The closeness centrality score can be determined by using the =closeness= function, the names of the nodes can be taken from the graph and then this can be sorted as shown in the listing [[gst]].

#+NAME: gst
#+CAPTION: Calculating the /Closeness Centrality Score/
#+begin_src R
## Inspect the Closness
closeness(g)[1:20]

## Sort the Closeness
index=order(closeness(g), decreasing = TRUE)
centrality_vector <- c(names(g[index][,1]), closeness(g)[index])

## Print the 3 most Central Users
centrality_vector[1:4]

## "Ubisoft" "BarackObama" "XboxSupport"    "DEADLINE"
#+end_src

** 8.3.27 List the Most Central People in the graph according to the closeness centrality
The most central people in order of closeness centrality are:

1. Ubisoft
2. Arbys
3. Xbox
4. GailSimone

  Unsurprisingly /Ubisoft/ is the most central person, this is expected because the graph was produced from the perspective of /Ubisoft/ and the nodes had a very limited degree of interconnectivity.


** 8.3.28 Comment on the Results
/Ubisoft/ has many connections as shown in figures [[gp1]] and [[ggpt]], these connections, however, have a limited amount of interconnectivity.

  Surprisingly, however, Gail Simone, a comic book author involved in works such as /Batgirl/ and /Birds of Prey/, is a very central character, in combination with the central connection to /Arbys/ this is indicative of the interests and lifestyle of the typical demographic of friends of the /Ubisoft/ account.
* Appendix

#+NAME: hfls
#+CAPTION: User ID and Friend Count of users with above highest friend count in sample
|         */User ID/* | */Friend Count/* |
|---------------------+------------------|
|           274488119 |             8752 |
|           743771665 |             5002 |
|          1036014247 |             4999 |
|          2281452613 |             4992 |
|          1554453560 |             4958 |
|  981233818408570880 |             4944 |
|  931765564388921344 |             4836 |
|           807405140 |             4710 |
| 1112579152970842112 |             4514 |
|          2441577446 |             4322 |
|           552692862 |             4229 |
|  956297007127252992 |             3976 |
|            22493896 |             3675 |
|           255922782 |             3500 |
| 1067409881332936709 |             3312 |
|            27998570 |             3210 |
|  715118521555017728 |             3099 |
|          2356170174 |             2885 |
|          2372688230 |             2880 |
|          1868357425 |             2719 |

#+NAME: lftb
#+CAPTION: User ID and Friend Count of users with above highest friend count in sample
 | */User ID/*           | */Friend Count/* |
 |-----------------------+------------------|
 | 1254280995592966145   |                0 |
 | 875126772978913280    |                0 |
 | 1254256124217319425   |                0 |
 | 1250219450210480128   |                0 |
 | 1214921087328411648   |                0 |
 | 1254115699628421120   |                0 |
 | 1217600080376520704   |                0 |
 | 1253480062453600257   |                0 |
 | 1254178435502571521   |                0 |
 | 1251955545092718592   |                0 |
 | 1106864828700712960   |                0 |
 | 1160744587620524032   |                0 |
 | 1254256536710504448   |                1 |
 | 1129040408384868352   |                1 |
 | 1254121201871589376   |                1 |
 | 1248687797755658243   |                2 |
 | 1210265263867932675   |                2 |
 | 3380784928            |                3 |
 | 1177274165239275520   |                3 |
 | 54645521              |                3 |



#+NAME: tfidf
#+CAPTION: Document Term Matrix of first 6 documents and first 6 Words
| **check** | **coronavirus** | **death** | **first** | **gone** | **hour** |
|-----------+-----------------+-----------+-----------+----------+----------|
|     0.615 |           0.747 |     0.830 |     0.596 |    0.747 |    0.698 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |
|         0 |               0 |         0 |         0 |        0 |        0 |

#+NAME: xkcd
#+CAPTION: /xkcd/ =#= 297
#+attr_html: :width 400px
#+attr_latex: :width 7cm
[[./Figures/lisp_cycles.png]]
* Footnotes

[fn:query]  The =rtweet= package will search for tweets that contain all the words of a query
regardless of uppercase or lowercase usage cite:kearney2019.



[fn:3] It is important not to filter based on the logic of an empty vector, because
otherwise an empty vector will returned, hence the if statement in listing [[emp]].

[fn:2] The =[= function is actually shorthand for =Extract()=, most things in
*/R/* are functions, this is similar to /LISP/ and has to do with the origins of
the language, e.g. ~sum(1:10) == (sum (1:10))~, also relevant see the relevant
xkcd in figure [[xkcd]].




[fn:1] This works because the =tm= package preserves the order of the data, this can be confirmed by using a data frame source as opposed to a vector source (e.g. in listing [[cpmk]]) and comparing the ID's before/after transformation.
# see ./scripts/q2.R


\newpage



# * References
# Remember, this is here for HTML and autocomplete, but latex uses biblatex for URL support
# bibliography:/home/ryan/Dropbox/Studies/Papers/references.bib
# I (Ryan) am managing this with zotero, please don't touch, I'll figure out how to sync the citations later or we can all just switch to a =.bib= file.
<<bibliography link>>
bibliography:./references.bib

<<bibliographystyle link>>
 bibliographystyle:unsrt
